{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#evalhub","title":"EvalHub","text":"<p>Open source evaluation orchestration platform for Large Language Models.</p>"},{"location":"#what-is-evalhub","title":"What is EvalHub?","text":"<p>EvalHub is an evaluation orchestration platform designed for systematic LLM evaluation. It supports both local development and Kubernetes-native deployment for scale. The platform consists of three components:</p> <ul> <li>EvalHub Server: REST API orchestration service for managing evaluation workflows</li> <li>EvalHub SDK: Python SDK for submitting evaluations and building adapters</li> <li>EvalHub Contrib: Community-contributed framework adapters</li> </ul>"},{"location":"#core-problem-solved","title":"Core Problem Solved","text":"<p>LLM evaluation involves coordinating multiple frameworks, managing evaluation jobs, tracking results, and handling diverse deployment environments. EvalHub solves this by providing:</p> <ul> <li>Unified evaluation API: Submit evaluations across frameworks using a consistent interface</li> <li>Kubernetes-native orchestration: Automatic job lifecycle management, scaling, and resource isolation</li> <li>Framework adapter pattern: \"Bring Your Own Framework\" (BYOF) approach with standardised integration</li> <li>Multi-environment support: Deploy locally for development or on OpenShift for production</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph client[\"Client Applications\"]\n        PY[Python SDK Client]\n        API[Direct REST API]\n    end\n\n    subgraph server[\"EvalHub Server\"]\n        REST[REST API Server]\n        ORCH[Job Orchestrator]\n        PROV[Provider Registry]\n    end\n\n    subgraph k8s[\"Kubernetes / OpenShift\"]\n        JOB[Evaluation Jobs]\n        ADAPT[Framework Adapters]\n        SIDE[Sidecar Containers]\n    end\n\n    subgraph storage[\"Storage\"]\n        OCI[OCI Registry]\n        DB[(PostgreSQL)]\n    end\n\n    PY --&gt; REST\n    API --&gt; REST\n    REST --&gt; ORCH\n    REST --&gt; PROV\n    ORCH --&gt; JOB\n    JOB --&gt; ADAPT\n    JOB --&gt; SIDE\n    ADAPT --&gt; SIDE\n    SIDE --&gt; OCI\n    REST --&gt; DB\n\n    style server fill:#e3f2fd\n    style k8s fill:#fff3e0\n    style storage fill:#f3e5f5</code></pre>"},{"location":"#component-responsibilities","title":"Component Responsibilities","text":"Component Description Technology Server REST API, job orchestration, provider management Go, SQLite (local) / PostgreSQL (production) SDK Client library, adapter framework, data models Python 3.12+ Contrib Community framework adapters (LightEval, GuideLLM) Python containers Jobs Isolated evaluation execution environments Kubernetes Jobs Registry Immutable artifact storage OCI registries"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#server-features","title":"Server Features","text":"<ul> <li>REST API: Versioned API (v1) with OpenAPI specification</li> <li>Provider management: Discover evaluation providers and benchmarks</li> <li>Collection management: Curated benchmark collections with weighted scoring</li> <li>Job orchestration: Kubernetes Job lifecycle management</li> <li>Persistent storage: SQLite for local development, PostgreSQL for production</li> <li>Prometheus metrics: Production-ready observability</li> </ul>"},{"location":"#sdk-features","title":"SDK Features","text":"<p>Client SDK (<code>evalhub.client</code>):</p> <ul> <li>Submit evaluations to EvalHub service</li> <li>Discover providers, benchmarks, and collections</li> <li>Monitor job status and retrieve results</li> <li>Async/await support for non-blocking workflows</li> </ul> <p>Adapter SDK (<code>evalhub.adapter</code>):</p> <ul> <li>Base class for framework integration</li> <li>Callback interface for status reporting</li> <li>OCI artifact persistence</li> <li>Settings-based configuration</li> </ul> <p>Core Models (<code>evalhub.models</code>):</p> <ul> <li>Shared data structures</li> <li>Request/response schemas</li> <li>API model validation</li> </ul>"},{"location":"#contrib-features","title":"Contrib Features","text":"<ul> <li>LightEval: Language model evaluation (HellaSwag, ARC, MMLU, TruthfulQA, GSM8K)</li> <li>GuideLLM: Performance benchmarking (TTFT, ITL, throughput, latency)</li> <li>Containerised: Production-ready container images</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#submit-an-evaluation-python-sdk","title":"Submit an Evaluation (Python SDK)","text":"<pre><code>from evalhub.client import EvalHubClient\nfrom evalhub.models.api import ModelConfig, BenchmarkSpec\n\n# Connect to EvalHub service\nclient = EvalHubClient(base_url=\"http://localhost:8080\")\n\n# Submit evaluation\njob = client.submit_evaluation(\n    model=ModelConfig(\n        url=\"https://api.openai.com/v1\",\n        name=\"gpt-4\"\n    ),\n    benchmarks=[\n        BenchmarkSpec(\n            benchmark_id=\"mmlu\",\n            provider_id=\"lm_evaluation_harness\"\n        )\n    ]\n)\n\n# Monitor progress\nstatus = client.get_job_status(job.job_id)\nprint(f\"Status: {status.status}, Progress: {status.progress}\")\n</code></pre>"},{"location":"#deploy-on-openshift","title":"Deploy on OpenShift","text":"<pre><code># Apply EvalHub manifests\noc apply -k config/openshift/\n\n# Verify deployment\noc get pods -n eval-hub\noc logs -f deployment/eval-hub-server\n</code></pre>"},{"location":"#build-a-framework-adapter","title":"Build a Framework Adapter","text":"<pre><code>from evalhub.adapter import FrameworkAdapter, JobSpec, JobResults, JobCallbacks\nfrom evalhub.adapter.models import JobStatusUpdate, JobStatus, JobPhase, OCIArtifactSpec\n\nclass MyAdapter(FrameworkAdapter):\n    def run_benchmark_job(\n        self,\n        config: JobSpec,\n        callbacks: JobCallbacks\n    ) -&gt; JobResults:\n        # Load framework\n        framework = load_your_framework()\n\n        # Report progress\n        callbacks.report_status(JobStatusUpdate(\n            status=JobStatus.RUNNING,\n            phase=JobPhase.RUNNING_EVALUATION,\n            progress=0.5,\n            message=\"Running evaluation\"\n        ))\n\n        # Run evaluation\n        results = framework.evaluate(config.benchmark_id)\n\n        # Persist artifacts\n        oci_result = callbacks.create_oci_artifact(OCIArtifactSpec(\n            files=[\"results.json\", \"report.html\"],\n            job_id=config.job_id,\n            benchmark_id=config.benchmark_id,\n            model_name=config.model.name\n        ))\n\n        # Return results\n        return JobResults(\n            job_id=config.job_id,\n            benchmark_id=config.benchmark_id,\n            model_name=config.model.name,\n            results=results.evaluation_results,\n            num_examples_evaluated=len(results.evaluation_results),\n            duration_seconds=results.duration,\n            oci_artifact=oci_result\n        )\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#model-evaluation","title":"Model Evaluation","text":"<p>Systematically evaluate LLMs across standardised benchmarks:</p> <pre><code># Evaluate model on multiple benchmarks\nclient.submit_evaluation(\n    model=ModelConfig(url=\"...\", name=\"llama-3-8b\"),\n    benchmarks=[\n        BenchmarkSpec(benchmark_id=\"mmlu\", provider_id=\"lm_evaluation_harness\"),\n        BenchmarkSpec(benchmark_id=\"humaneval\", provider_id=\"lm_evaluation_harness\"),\n        BenchmarkSpec(benchmark_id=\"gsm8k\", provider_id=\"lm_evaluation_harness\"),\n    ]\n)\n</code></pre>"},{"location":"#performance-testing","title":"Performance Testing","text":"<p>Benchmark inference server performance under load:</p> <pre><code># Test server throughput and latency\nclient.submit_evaluation(\n    model=ModelConfig(url=\"http://vllm-server:8000\", name=\"llama-3-8b\"),\n    benchmarks=[\n        BenchmarkSpec(\n            benchmark_id=\"performance_test\",\n            provider_id=\"guidellm\",\n            config={\n                \"profile\": \"constant\",\n                \"rate\": 10,\n                \"max_seconds\": 60\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"#collection-based-evaluation","title":"Collection-Based Evaluation","text":"<p>Run curated benchmark collections:</p> <pre><code># Evaluate using predefined collection\nclient.submit_evaluation(\n    model=ModelConfig(url=\"...\", name=\"gpt-4\"),\n    collection_id=\"healthcare_safety_v1\"  # Expands to multiple benchmarks\n)\n</code></pre>"},{"location":"#community","title":"Community","text":""},{"location":"#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Documentation: This site</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 - see LICENSE for details.</p>"},{"location":"adapters/guidellm/","title":"GuideLLM Adapter","text":""},{"location":"adapters/guidellm/#guidellm-adapter","title":"GuideLLM Adapter","text":"<p>The GuideLLM adapter integrates GuideLLM with the eval-hub evaluation service using the evalhub-sdk framework adapter pattern.</p>"},{"location":"adapters/guidellm/#overview","title":"Overview","text":"<p>GuideLLM is a performance benchmarking platform designed to evaluate language model inference servers under realistic production conditions.</p>"},{"location":"adapters/guidellm/#key-features","title":"Key Features","text":"<ul> <li>Multiple execution profiles: Sweep, throughput, concurrent, constant, poisson, synchronous</li> <li>Comprehensive metrics: Time to First Token (TTFT), Inter-Token Latency (ITL), end-to-end latency, throughput</li> <li>Flexible data sources: Synthetic data generation, HuggingFace datasets, local files</li> <li>Rich reporting: JSON, CSV, HTML, and YAML output formats with detailed visualisations</li> </ul>"},{"location":"adapters/guidellm/#supported-backends","title":"Supported Backends","text":"<ul> <li>OpenAI-compatible endpoints (vLLM, Text Generation Inference, etc.)</li> <li>Any HTTP API following OpenAI's chat completions or completions format</li> </ul>"},{"location":"adapters/guidellm/#architecture","title":"Architecture","text":"<p>The adapter follows the eval-hub framework adapter pattern with automatic configuration:</p> <pre><code>graph TD\n    A[ConfigMap&lt;br/&gt;JobSpec] --&gt;|mounted at /meta/job.json| B[Kubernetes Job Pod]\n    B --&gt; C[Adapter Container]\n    B --&gt; D[Sidecar Container]\n\n    C --&gt;|1. Load JobSpec| C\n    C --&gt;|2. Run GuideLLM| C\n    C --&gt;|3. Parse Results| C\n    C --&gt;|4. Report via callbacks| D\n\n    D --&gt;|Receive status| D\n    D --&gt;|Persist artifacts| D\n    D --&gt;|Store in OCI registry| E[OCI Registry]</code></pre> <p>Workflow:</p> <ol> <li>Settings-based configuration: Runtime settings loaded automatically from environment</li> <li>Automatic JobSpec loading: Job configuration auto-loaded from mounted ConfigMap</li> <li>Callback-based communication: Progress updates and artifacts sent to sidecar via callbacks</li> <li>Synchronous execution: The entire job lifetime is defined by the <code>run_benchmark_job()</code> method</li> <li>OCI artifact persistence: Results persisted as OCI artifacts via the sidecar</li> <li>Structured results: Returns <code>JobResults</code> with standardised performance metrics</li> </ol>"},{"location":"adapters/guidellm/#quick-start","title":"Quick Start","text":""},{"location":"adapters/guidellm/#building-the-container","title":"Building the Container","text":"<pre><code>make image-guidellm\n</code></pre>"},{"location":"adapters/guidellm/#running-locally","title":"Running Locally","text":"<p>For local testing without Kubernetes:</p> Basic UsageWith Ollama <pre><code># Set environment for local mode\nexport EVALHUB_MODE=local\nexport EVALHUB_JOB_SPEC_PATH=meta/job.json\nexport SERVICE_URL=http://localhost:8080  # Optional: if mock service is running\n\n# Run the adapter\npython main.py\n</code></pre> <pre><code># Start Ollama and pull a model\nollama run qwen2.5:1.5b\n\n# Run benchmark against Ollama\nexport EVALHUB_MODE=local\nexport EVALHUB_JOB_SPEC_PATH=meta/job.json\npython main.py\n</code></pre>"},{"location":"adapters/guidellm/#container-image","title":"Container Image","text":"<pre><code># Pull from registry\npodman pull quay.io/eval-hub/community-guidellm:latest\n\n# Run with custom job spec\npodman run \\\n  -e EVALHUB_MODE=local \\\n  -e EVALHUB_JOB_SPEC_PATH=/meta/job.json \\\n  -v $(pwd)/job.json:/meta/job.json:ro \\\n  quay.io/eval-hub/community-guidellm:latest\n</code></pre>"},{"location":"adapters/guidellm/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Options - Detailed configuration reference</li> <li>Execution Profiles - Different load testing patterns</li> <li>Metrics - Understanding performance metrics</li> <li>Examples - Complete configuration examples</li> </ul>"},{"location":"adapters/guidellm/configuration/","title":"Configuration Reference","text":""},{"location":"adapters/guidellm/configuration/#configuration-reference","title":"Configuration Reference","text":"<p>Complete reference for GuideLLM adapter configuration options.</p>"},{"location":"adapters/guidellm/configuration/#jobspec-structure","title":"JobSpec Structure","text":"<p>The GuideLLM adapter uses a standardised <code>JobSpec</code> structure:</p> <pre><code>{\n  \"job_id\": \"string\",\n  \"benchmark_id\": \"string\",\n  \"model\": {\n    \"name\": \"string\",\n    \"url\": \"string\"\n  },\n  \"benchmark_config\": {\n    // GuideLLM-specific configuration\n  },\n  \"experiment_name\": \"string\",\n  \"tags\": {},\n  \"timeout_seconds\": 60\n}\n</code></pre>"},{"location":"adapters/guidellm/configuration/#core-parameters","title":"Core Parameters","text":""},{"location":"adapters/guidellm/configuration/#required-parameters","title":"Required Parameters","text":"Parameter Type Description Example <code>job_id</code> string Unique job identifier <code>\"guidellm-001\"</code> <code>benchmark_id</code> string Benchmark identifier <code>\"performance_sweep\"</code> <code>model.name</code> string Model name <code>\"Qwen/Qwen2.5-1.5B-Instruct\"</code> <code>model.url</code> string OpenAI-compatible API endpoint <code>\"http://localhost:8000/v1\"</code>"},{"location":"adapters/guidellm/configuration/#optional-parameters","title":"Optional Parameters","text":"Parameter Type Description Default <code>experiment_name</code> string Experiment identifier <code>null</code> <code>tags</code> object Free-form metadata tags <code>{}</code> <code>timeout_seconds</code> integer Job timeout <code>60</code>"},{"location":"adapters/guidellm/configuration/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>All configuration is specified in the <code>benchmark_config</code> object.</p>"},{"location":"adapters/guidellm/configuration/#execution-profile","title":"Execution Profile","text":"Parameter Type Description Options <code>profile</code> string Execution profile <code>sweep</code>, <code>throughput</code>, <code>concurrent</code>, <code>constant</code>, <code>poisson</code>, <code>synchronous</code> <p>See Execution Profiles for detailed information on each profile type.</p>"},{"location":"adapters/guidellm/configuration/#rate-configuration","title":"Rate Configuration","text":"Parameter Type Description Varies by Profile <code>rate</code> number or array Request rate configuration Profile-dependent <p>Profile-specific behaviour:</p> <ul> <li>sweep: Not used (automatically determined)</li> <li>throughput: Not used (maximum speed)</li> <li>concurrent: Number of concurrent requests</li> <li>constant: Requests per second</li> <li>poisson: Average requests per second</li> <li>synchronous: Not used (sequential)</li> </ul>"},{"location":"adapters/guidellm/configuration/#duration-and-limits","title":"Duration and Limits","text":"Parameter Type Description Default <code>max_seconds</code> number Maximum duration in seconds None (unlimited) <code>max_requests</code> number Maximum number of requests None (unlimited) <code>max_errors</code> number Error threshold before stopping None (unlimited) <code>max_error_rate</code> number Error rate threshold (0-1) None <code>max_global_error_rate</code> number Global error rate threshold None <p>At Least One Limit Required</p> <p>You must specify at least one of: <code>max_seconds</code>, <code>max_requests</code>, or error limits.</p>"},{"location":"adapters/guidellm/configuration/#warmup-and-cooldown","title":"Warmup and Cooldown","text":"Parameter Type Description Example <code>warmup</code> string or number Warmup period to exclude <code>\"5%\"</code> or <code>10</code> <code>cooldown</code> string or number Cooldown period to exclude <code>\"5%\"</code> or <code>10</code> <p>Format: - Percentage: <code>\"5%\"</code> - exclude first/last 5% of requests - Absolute: <code>10</code> - exclude first/last 10 seconds</p> <p>Recommended Warmup</p> <p>Use <code>\"warmup\": \"5%\"</code> to exclude cold-start effects from measurements.</p>"},{"location":"adapters/guidellm/configuration/#saturation-detection","title":"Saturation Detection","text":"Parameter Type Description Default <code>detect_saturation</code> boolean Enable over-saturation detection <code>false</code> <code>over_saturation</code> number Saturation threshold multiplier <code>1.5</code> <p>When enabled, automatically detects when the server is saturated and adjusts testing accordingly.</p>"},{"location":"adapters/guidellm/configuration/#data-sources","title":"Data Sources","text":""},{"location":"adapters/guidellm/configuration/#synthetic-data","title":"Synthetic Data","text":"<p>Generate synthetic requests with specified token counts:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"data\": \"prompt_tokens=50,output_tokens=20\"\n  }\n}\n</code></pre> <p>Format: <code>prompt_tokens=N,output_tokens=M</code></p>"},{"location":"adapters/guidellm/configuration/#huggingface-datasets","title":"HuggingFace Datasets","text":"<p>Use datasets from HuggingFace:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"data\": \"hf:abisee/cnn_dailymail\",\n    \"data_args\": {\"name\": \"3.0.0\"},\n    \"data_column_mapper\": {\"text_column\": \"article\"},\n    \"data_samples\": 100\n  }\n}\n</code></pre> Parameter Type Description <code>data</code> string Dataset identifier (prefix with <code>hf:</code>) <code>data_args</code> object Dataset loading arguments <code>data_column_mapper</code> object Column name mappings <code>data_samples</code> number Maximum samples to use"},{"location":"adapters/guidellm/configuration/#local-files","title":"Local Files","text":"<p>Use local data files:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"data\": \"file:///path/to/prompts.jsonl\",\n    \"data_samples\": 500\n  }\n}\n</code></pre> <p>Supported formats: JSON, JSONL, CSV</p>"},{"location":"adapters/guidellm/configuration/#data-processing","title":"Data Processing","text":"Parameter Type Description Example <code>processor</code> string Tokeniser for synthetic data <code>\"gpt2\"</code> <code>processor_args</code> array Processor arguments <code>[]</code> <code>data_num_workers</code> number Parallel workers for data loading <code>1</code>"},{"location":"adapters/guidellm/configuration/#request-configuration","title":"Request Configuration","text":""},{"location":"adapters/guidellm/configuration/#request-type","title":"Request Type","text":"Parameter Type Description Options <code>request_type</code> string API endpoint type <code>chat_completions</code>, <code>completions</code>, <code>audio_transcription</code>, <code>audio_translation</code> <p>Default: <code>chat_completions</code></p>"},{"location":"adapters/guidellm/configuration/#request-formatting","title":"Request Formatting","text":"Parameter Type Description Options <code>data_request_formatter</code> string Request format <code>chat_completions</code>, <code>completions</code> <code>data_collator</code> string Data collation strategy <code>generative</code>"},{"location":"adapters/guidellm/configuration/#output-configuration","title":"Output Configuration","text":""},{"location":"adapters/guidellm/configuration/#output-formats","title":"Output Formats","text":"Parameter Type Description Default <code>outputs</code> array Output formats <code>[\"json\", \"csv\", \"html\", \"yaml\"]</code> <code>output_dir</code> string Output directory <code>/tmp/guidellm_results_*</code>"},{"location":"adapters/guidellm/configuration/#advanced-options","title":"Advanced Options","text":""},{"location":"adapters/guidellm/configuration/#randomisation","title":"Randomisation","text":"Parameter Type Description Default <code>random_seed</code> number Random seed for reproducibility <code>42</code>"},{"location":"adapters/guidellm/configuration/#backend-configuration","title":"Backend Configuration","text":"Parameter Type Description Default <code>backend</code> string Backend type <code>openai_http</code> <code>backend_kwargs</code> object Additional backend arguments <code>null</code>"},{"location":"adapters/guidellm/configuration/#environment-variables","title":"Environment Variables","text":"<p>The adapter reads runtime settings from environment variables:</p> Variable Description Required Default <code>EVALHUB_MODE</code> Execution mode No <code>k8s</code> <code>EVALHUB_JOB_SPEC_PATH</code> Path to job spec JSON Yes (local mode) <code>/meta/job.json</code> (k8s), <code>meta/job.json</code> (local) <code>SERVICE_URL</code> Eval-hub service URL No <code>null</code> <code>REGISTRY_URL</code> OCI registry URL No <code>null</code> <code>REGISTRY_USERNAME</code> Registry username No <code>null</code> <code>REGISTRY_PASSWORD</code> Registry password No <code>null</code> <code>REGISTRY_INSECURE</code> Allow insecure registry No <code>false</code>"},{"location":"adapters/guidellm/configuration/#complete-example","title":"Complete Example","text":"<pre><code>{\n  \"job_id\": \"guidellm-production-001\",\n  \"benchmark_id\": \"performance_sweep\",\n  \"model\": {\n    \"name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n    \"url\": \"http://127.0.0.1:8000/v1\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 5,\n    \"max_seconds\": 60,\n    \"max_requests\": 100,\n    \"data\": \"prompt_tokens=256,output_tokens=128\",\n    \"request_type\": \"chat_completions\",\n    \"warmup\": \"5%\",\n    \"detect_saturation\": true,\n    \"random_seed\": 42\n  },\n  \"experiment_name\": \"qwen-load-test\",\n  \"tags\": {\n    \"framework\": \"guidellm\",\n    \"model_size\": \"small\",\n    \"evaluation_type\": \"performance\"\n  },\n  \"timeout_seconds\": 300\n}\n</code></pre>"},{"location":"adapters/guidellm/examples/","title":"Configuration Examples","text":""},{"location":"adapters/guidellm/examples/#configuration-examples","title":"Configuration Examples","text":"<p>Complete examples for common GuideLLM benchmarking scenarios.</p>"},{"location":"adapters/guidellm/examples/#quick-test","title":"Quick Test","text":"<p>Fast validation with minimal samples:</p> <pre><code>{\n  \"job_id\": \"guidellm-quick-001\",\n  \"benchmark_id\": \"performance_quick\",\n  \"model\": {\n    \"url\": \"http://127.0.0.1:8000/v1\",\n    \"name\": \"Qwen/Qwen2.5-1.5B-Instruct\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 5,\n    \"max_seconds\": 10,\n    \"max_requests\": 20,\n    \"data\": \"prompt_tokens=50,output_tokens=20\",\n    \"request_type\": \"chat_completions\",\n    \"warmup\": \"0\",\n    \"detect_saturation\": false\n  },\n  \"experiment_name\": \"qwen-quick-test\",\n  \"tags\": {\n    \"framework\": \"guidellm\",\n    \"model_size\": \"small\",\n    \"evaluation_type\": \"performance\"\n  },\n  \"timeout_seconds\": 60\n}\n</code></pre> <p>Duration: ~10 seconds Use case: Quick validation, CI/CD testing</p>"},{"location":"adapters/guidellm/examples/#performance-sweep","title":"Performance Sweep","text":"<p>Automatically explore request rates:</p> <pre><code>{\n  \"job_id\": \"guidellm-sweep-001\",\n  \"benchmark_id\": \"performance_sweep\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"Qwen/Qwen2.5-1.5B-Instruct\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"sweep\",\n    \"max_seconds\": 30,\n    \"max_requests\": 100,\n    \"data\": \"prompt_tokens=256,output_tokens=128\",\n    \"warmup\": \"5%\",\n    \"detect_saturation\": true\n  },\n  \"experiment_name\": \"qwen-capacity-discovery\",\n  \"tags\": {\n    \"test_type\": \"discovery\",\n    \"purpose\": \"capacity_planning\"\n  },\n  \"timeout_seconds\": 120\n}\n</code></pre> <p>Duration: ~30 seconds Use case: Initial capacity discovery, finding safe operating range</p>"},{"location":"adapters/guidellm/examples/#constant-load-test","title":"Constant Load Test","text":"<p>Steady-state performance measurement:</p> <pre><code>{\n  \"job_id\": \"guidellm-constant-001\",\n  \"benchmark_id\": \"performance_constant\",\n  \"model\": {\n    \"url\": \"http://production.example.com/v1\",\n    \"name\": \"llama-2-7b-chat\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 10,\n    \"max_seconds\": 300,\n    \"max_requests\": 3000,\n    \"data\": \"hf:abisee/cnn_dailymail\",\n    \"data_args\": {\"name\": \"3.0.0\"},\n    \"data_column_mapper\": {\"text_column\": \"article\"},\n    \"data_samples\": 500,\n    \"warmup\": \"5%\",\n    \"cooldown\": \"5%\"\n  },\n  \"experiment_name\": \"llama-production-baseline\",\n  \"tags\": {\n    \"environment\": \"production\",\n    \"test_type\": \"baseline\"\n  },\n  \"timeout_seconds\": 600\n}\n</code></pre> <p>Duration: 5 minutes Use case: Production baseline, SLA validation</p>"},{"location":"adapters/guidellm/examples/#throughput-test","title":"Throughput Test","text":"<p>Maximum capacity testing:</p> <pre><code>{\n  \"job_id\": \"guidellm-throughput-001\",\n  \"benchmark_id\": \"max_throughput\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"gpt-3.5-turbo\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"throughput\",\n    \"max_seconds\": 60,\n    \"max_requests\": 5000,\n    \"data\": \"prompt_tokens=512,output_tokens=256\",\n    \"warmup\": \"10%\",\n    \"max_error_rate\": 0.1\n  },\n  \"experiment_name\": \"gpt35-max-capacity\",\n  \"tags\": {\n    \"test_type\": \"stress\",\n    \"purpose\": \"capacity_limit\"\n  },\n  \"timeout_seconds\": 180\n}\n</code></pre> <p>Duration: 1 minute + warmup Use case: Stress testing, capacity planning</p>"},{"location":"adapters/guidellm/examples/#concurrent-users","title":"Concurrent Users","text":"<p>Simulate parallel user load:</p> <pre><code>{\n  \"job_id\": \"guidellm-concurrent-001\",\n  \"benchmark_id\": \"concurrent_users\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"llama-2-13b\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"concurrent\",\n    \"rate\": 25,\n    \"max_requests\": 500,\n    \"max_seconds\": 120,\n    \"data\": \"prompt_tokens=512,output_tokens=256\",\n    \"warmup\": \"5%\"\n  },\n  \"experiment_name\": \"llama-concurrent-load\",\n  \"tags\": {\n    \"test_type\": \"concurrency\",\n    \"concurrent_users\": 25\n  },\n  \"timeout_seconds\": 300\n}\n</code></pre> <p>Duration: 2 minutes Use case: User simulation, concurrency testing</p>"},{"location":"adapters/guidellm/examples/#poisson-distribution","title":"Poisson Distribution","text":"<p>Realistic production traffic pattern:</p> <pre><code>{\n  \"job_id\": \"guidellm-poisson-001\",\n  \"benchmark_id\": \"poisson_traffic\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"mistral-7b\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"poisson\",\n    \"rate\": 15,\n    \"max_seconds\": 180,\n    \"data\": \"hf:openai/gsm8k\",\n    \"data_args\": {\"name\": \"main\"},\n    \"data_column_mapper\": {\"text_column\": \"question\"},\n    \"data_samples\": 1000,\n    \"warmup\": \"5%\",\n    \"detect_saturation\": true\n  },\n  \"experiment_name\": \"mistral-realistic-load\",\n  \"tags\": {\n    \"test_type\": \"realistic\",\n    \"traffic_pattern\": \"poisson\"\n  },\n  \"timeout_seconds\": 400\n}\n</code></pre> <p>Duration: 3 minutes Use case: Production simulation, realistic load testing</p>"},{"location":"adapters/guidellm/examples/#synchronous-baseline","title":"Synchronous Baseline","text":"<p>Single-user minimum latency:</p> <pre><code>{\n  \"job_id\": \"guidellm-sync-001\",\n  \"benchmark_id\": \"baseline_latency\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"qwen-1.5b\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"synchronous\",\n    \"max_requests\": 100,\n    \"data\": \"prompt_tokens=256,output_tokens=128\"\n  },\n  \"experiment_name\": \"qwen-baseline\",\n  \"tags\": {\n    \"test_type\": \"baseline\",\n    \"load\": \"single_user\"\n  },\n  \"timeout_seconds\": 300\n}\n</code></pre> <p>Duration: Variable (depends on model speed) Use case: Baseline measurement, minimum latency testing</p>"},{"location":"adapters/guidellm/examples/#local-testing-with-ollama","title":"Local Testing with Ollama","text":"<p>Configuration for local testing with Ollama:</p> Start OllamaJob SpecificationRun Benchmark <pre><code># Install Ollama (if not already installed)\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull and run a model\nollama run qwen2.5:1.5b\n</code></pre> <pre><code>{\n  \"job_id\": \"local-test-001\",\n  \"benchmark_id\": \"ollama_test\",\n  \"model\": {\n    \"url\": \"http://localhost:11434/v1\",\n    \"name\": \"qwen2.5:1.5b\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 5,\n    \"max_seconds\": 10,\n    \"max_requests\": 20,\n    \"data\": \"prompt_tokens=50,output_tokens=20\",\n    \"warmup\": \"0\"\n  }\n}\n</code></pre> <pre><code># Set environment\nexport EVALHUB_MODE=local\nexport EVALHUB_JOB_SPEC_PATH=meta/job.json\nexport SERVICE_URL=http://localhost:8080\n\n# Run adapter\npython main.py\n</code></pre>"},{"location":"adapters/guidellm/examples/#error-handling","title":"Error Handling","text":"<p>Configuration with error thresholds:</p> <pre><code>{\n  \"job_id\": \"guidellm-resilience-001\",\n  \"benchmark_id\": \"error_tolerance\",\n  \"model\": {\n    \"url\": \"http://localhost:8000/v1\",\n    \"name\": \"test-model\"\n  },\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 10,\n    \"max_seconds\": 60,\n    \"max_errors\": 10,\n    \"max_error_rate\": 0.05,\n    \"data\": \"prompt_tokens=256,output_tokens=128\"\n  },\n  \"experiment_name\": \"error-tolerance-test\",\n  \"tags\": {\n    \"test_type\": \"resilience\"\n  },\n  \"timeout_seconds\": 120\n}\n</code></pre> <p>Stops when: - 10 total errors occur, OR - Error rate exceeds 5%</p>"},{"location":"adapters/guidellm/examples/#tips-for-writing-configurations","title":"Tips for Writing Configurations","text":""},{"location":"adapters/guidellm/examples/#choose-the-right-profile","title":"Choose the Right Profile","text":"<ul> <li>First test: Use <code>sweep</code> to discover safe rates</li> <li>Repeatable tests: Use <code>constant</code> for consistent results</li> <li>Stress tests: Use <code>throughput</code> to find limits</li> <li>Production simulation: Use <code>poisson</code> for realistic traffic</li> </ul>"},{"location":"adapters/guidellm/examples/#set-appropriate-limits","title":"Set Appropriate Limits","text":"<ul> <li>Always specify at least one: <code>max_seconds</code>, <code>max_requests</code></li> <li>Use <code>max_error_rate</code> to fail fast on issues</li> <li>Add <code>warmup</code> to exclude cold-start effects</li> </ul>"},{"location":"adapters/guidellm/examples/#data-sources","title":"Data Sources","text":"<ul> <li>Quick tests: Use synthetic data (<code>prompt_tokens=N,output_tokens=M</code>)</li> <li>Realistic tests: Use HuggingFace datasets (<code>hf:dataset_name</code>)</li> <li>Specific scenarios: Use local files (<code>file:///path/to/data</code>)</li> </ul>"},{"location":"adapters/guidellm/examples/#warmup-best-practices","title":"Warmup Best Practices","text":"<pre><code>{\n  \"warmup\": \"5%\",    // For percentage-based\n  \"warmup\": 10       // For time-based (seconds)\n}\n</code></pre> <p>Recommended: <code>\"5%\"</code> for most tests</p>"},{"location":"adapters/guidellm/examples/#tags-for-organisation","title":"Tags for Organisation","text":"<p>Use tags to categorise benchmarks:</p> <pre><code>{\n  \"tags\": {\n    \"environment\": \"production|staging|dev\",\n    \"test_type\": \"baseline|stress|discovery\",\n    \"model_size\": \"small|medium|large\",\n    \"purpose\": \"capacity_planning|sla_validation|regression\"\n  }\n}\n</code></pre>"},{"location":"adapters/guidellm/metrics/","title":"Performance Metrics","text":""},{"location":"adapters/guidellm/metrics/#performance-metrics","title":"Performance Metrics","text":"<p>GuideLLM collects comprehensive performance metrics for LLM inference evaluation.</p>"},{"location":"adapters/guidellm/metrics/#core-metrics","title":"Core Metrics","text":""},{"location":"adapters/guidellm/metrics/#requests-per-second","title":"Requests Per Second","text":"<p>Definition: Number of successful requests processed per second</p> <p>Use case: Overall system throughput measurement</p> <p>Typical values: - Small models (1-3B): 10-50 req/s - Medium models (7-13B): 3-15 req/s - Large models (30B+): 1-5 req/s</p> <p>Extracted metric: <code>requests_per_second</code></p>"},{"location":"adapters/guidellm/metrics/#time-to-first-token-ttft","title":"Time to First Token (TTFT)","text":"<p>Definition: Latency from request submission until the first generated token</p> <p>Use case: User experience - measures perceived responsiveness</p> <p>Typical values: - Fast: &lt; 100ms - Good: 100-500ms - Slow: &gt; 1000ms</p> <p>Extracted metric: <code>mean_ttft_ms</code></p> <p>Important for: Interactive applications, chatbots, real-time systems</p>"},{"location":"adapters/guidellm/metrics/#inter-token-latency-itl","title":"Inter-Token Latency (ITL)","text":"<p>Definition: Time between consecutive generated tokens</p> <p>Use case: Streaming quality - measures generation smoothness</p> <p>Typical values: - Fast: &lt; 20ms - Good: 20-50ms - Slow: &gt; 100ms</p> <p>Extracted metric: <code>mean_itl_ms</code></p> <p>Important for: Streaming responses, user experience</p>"},{"location":"adapters/guidellm/metrics/#token-throughput","title":"Token Throughput","text":"<p>Definition: Tokens generated per second</p> <p>Types: - Prompt tokens/sec: Input processing rate - Output tokens/sec: Generation rate - Total tokens/sec: Combined throughput</p> <p>Extracted metrics: - <code>prompt_tokens_per_second</code> - <code>output_tokens_per_second</code></p> <p>Use case: Cost estimation, capacity planning</p>"},{"location":"adapters/guidellm/metrics/#request-latency","title":"Request Latency","text":"<p>Definition: End-to-end time from request to complete response</p> <p>Calculation: <code>TTFT + (num_tokens \u00d7 ITL)</code></p> <p>Use case: Overall performance measurement</p> <p>Typical values: - Interactive: &lt; 2s - Batch processing: 5-30s</p>"},{"location":"adapters/guidellm/metrics/#total-requests","title":"Total Requests","text":"<p>Definition: Count of successful requests processed</p> <p>Extracted metric: <code>total_requests</code></p> <p>Use case: Validation, sample size verification</p>"},{"location":"adapters/guidellm/metrics/#statistical-measures","title":"Statistical Measures","text":"<p>All metrics include statistical measures:</p> <ul> <li>Mean: Average value</li> <li>Median: Middle value (50th percentile)</li> <li>Standard Deviation: Variability measure</li> <li>Percentiles: Distribution (p50, p75, p90, p95, p99)</li> </ul>"},{"location":"adapters/guidellm/metrics/#metric-extraction","title":"Metric Extraction","text":"<p>The adapter extracts summary statistics from GuideLLM's nested output structure:</p> <pre><code>{\n  \"framework\": \"guidellm\",\n  \"benchmark_id\": \"performance_quick\",\n  \"requests_per_second\": 4.99,\n  \"prompt_tokens_per_second\": 263.17,\n  \"output_tokens_per_second\": 105.27,\n  \"mean_ttft_ms\": 0.0,\n  \"mean_itl_ms\": 0.0,\n  \"total_requests\": 20,\n  \"benchmark_count\": 1\n}\n</code></pre>"},{"location":"adapters/guidellm/metrics/#interpreting-results","title":"Interpreting Results","text":""},{"location":"adapters/guidellm/metrics/#good-performance-indicators","title":"Good Performance Indicators","text":"<p>\u2713 Low TTFT (&lt; 200ms) - Responsive feel \u2713 Consistent ITL (low std dev) - Smooth streaming \u2713 High token throughput - Efficient generation \u2713 Stable request rate - No saturation</p>"},{"location":"adapters/guidellm/metrics/#performance-issues","title":"Performance Issues","text":"<p>\u26a0 High TTFT (&gt; 1s) - Poor responsiveness \u26a0 Variable ITL (high std dev) - Stuttering generation \u26a0 Low throughput - Under-utilised resources \u26a0 Increasing latency - Approaching saturation</p>"},{"location":"adapters/guidellm/metrics/#benchmark-output","title":"Benchmark Output","text":"<p>GuideLLM generates multiple output formats with detailed metrics:</p>"},{"location":"adapters/guidellm/metrics/#json-output","title":"JSON Output","text":"<p>Complete authoritative record with all metrics and sample requests.</p> <p>File: <code>benchmarks.json</code></p> <p>Contains: - All statistical measures - Request-level data - System metadata - Configuration details</p>"},{"location":"adapters/guidellm/metrics/#csv-output","title":"CSV Output","text":"<p>Tabular view for spreadsheets and BI tools.</p> <p>File: <code>benchmarks.csv</code></p> <p>Columns: All metrics flattened with mean/median/std/percentiles</p>"},{"location":"adapters/guidellm/metrics/#html-output","title":"HTML Output","text":"<p>Visual summary with latency distributions and interactive charts.</p> <p>File: <code>benchmarks.html</code></p> <p>Includes: - Performance summary tables - Latency distribution graphs - Token throughput visualisations - Request timeline</p>"},{"location":"adapters/guidellm/metrics/#yaml-output","title":"YAML Output","text":"<p>Human-readable alternative to JSON format.</p> <p>File: <code>benchmarks.yaml</code></p> <p>Use case: Configuration review, documentation</p>"},{"location":"adapters/guidellm/metrics/#example-output","title":"Example Output","text":"<p>Here's a sample benchmark result:</p> <pre><code>Token Metrics (Completed Requests)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Benchmark  \u2502 Prompt Tokens  \u2502\u2502 Generated Tokens \u2502\u2502 Total Tokens   \u2502\u2502 Iterations       \u2502\u2502\n\u2502 Strategy   \u2502 Per Request    \u2502\u2502 Per Request      \u2502\u2502 Per Request    \u2502\u2502 Per Request      \u2502\u2502\n\u2502            \u2502 Mdn  \u2502 p95  \u2502 Mdn  \u2502 p95  \u2502 Mdn  \u2502 p95  \u2502 Mdn   \u2502 p95  \u2502 Mdn     \u2502 p95    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 constant   \u2502 50.0 \u2502 50.0 \u2502 20.0 \u2502 20.0 \u2502 70.0 \u2502 70.0 \u2502 1.0   \u2502 1.0  \u2502 20.0    \u2502 20.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nServer Throughput Statistics\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Benchmark  \u2502 Requests      \u2502\u2502 Input Tokens \u2502\u2502 Output Tokens \u2502\u2502 Total Tokens      \u2502\u2502\n\u2502 Strategy   \u2502 Per Sec       \u2502\u2502 Per Sec      \u2502\u2502 Per Sec       \u2502\u2502 Per Sec           \u2502\u2502\n\u2502            \u2502 Mdn  \u2502 Mean  \u2502 Mdn    \u2502 Mean  \u2502 Mdn     \u2502 Mean  \u2502 Mdn     \u2502 Mean    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 constant   \u2502 5.0  \u2502 5.0   \u2502 250.3  \u2502 263.2 \u2502 100.1   \u2502 105.3  \u2502 350.5   \u2502 368.4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"adapters/guidellm/metrics/#metric-persistence","title":"Metric Persistence","text":"<p>All metrics are:</p> <ol> <li>Sent to eval-hub service: Summary metrics for tracking and comparison</li> <li>Persisted as OCI artifacts: Complete results for detailed analysis</li> <li>Logged: Real-time visibility during benchmark execution</li> </ol> <p>Payload Optimization</p> <p>The adapter sends only summary metrics to the service to reduce payload size. Full raw results are available in the OCI artifacts.</p>"},{"location":"adapters/guidellm/profiles/","title":"Execution Profiles","text":""},{"location":"adapters/guidellm/profiles/#execution-profiles","title":"Execution Profiles","text":"<p>GuideLLM supports multiple load patterns for different testing scenarios.</p>"},{"location":"adapters/guidellm/profiles/#profile-types","title":"Profile Types","text":""},{"location":"adapters/guidellm/profiles/#sweep-profile","title":"Sweep Profile","text":"<p>Automatically explore different request rates to find safe operating ranges.</p> <p>Use case: Discovery - find optimal request rates for your deployment</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"sweep\",\n    \"max_seconds\": 30,\n    \"detect_saturation\": true\n  }\n}\n</code></pre> <p>Behaviour: Incrementally increases request rate until saturation or limits are reached.</p>"},{"location":"adapters/guidellm/profiles/#throughput-profile","title":"Throughput Profile","text":"<p>Maximum capacity testing to identify performance limits.</p> <p>Use case: Stress testing - find the breaking point</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"throughput\",\n    \"max_seconds\": 60,\n    \"max_requests\": 1000\n  }\n}\n</code></pre> <p>Behaviour: Sends requests as fast as possible to saturate the server.</p>"},{"location":"adapters/guidellm/profiles/#concurrent-profile","title":"Concurrent Profile","text":"<p>Simulate parallel users with fixed concurrency level.</p> <p>Use case: User simulation - test with realistic concurrent load</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"concurrent\",\n    \"rate\": 10,\n    \"max_requests\": 100\n  }\n}\n</code></pre> <p>Behaviour: Maintains exactly N concurrent requests at all times.</p>"},{"location":"adapters/guidellm/profiles/#constant-profile","title":"Constant Profile","text":"<p>Fixed requests per second for steady-state testing.</p> <p>Use case: Baseline measurement - consistent, predictable load</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 5,\n    \"max_seconds\": 10,\n    \"max_requests\": 20\n  }\n}\n</code></pre> <p>Behaviour: Sends requests at a fixed rate (e.g., 5 req/s).</p>"},{"location":"adapters/guidellm/profiles/#poisson-profile","title":"Poisson Profile","text":"<p>Randomised request rates following Poisson distribution.</p> <p>Use case: Realistic simulation - natural traffic patterns</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"poisson\",\n    \"rate\": 5,\n    \"max_seconds\": 30\n  }\n}\n</code></pre> <p>Behaviour: Random intervals averaging to the specified rate.</p>"},{"location":"adapters/guidellm/profiles/#synchronous-profile","title":"Synchronous Profile","text":"<p>Sequential requests for baseline measurements.</p> <p>Use case: Single-user testing - minimum latency baseline</p> <p>Configuration:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"synchronous\",\n    \"max_requests\": 50\n  }\n}\n</code></pre> <p>Behaviour: Waits for each request to complete before sending the next.</p>"},{"location":"adapters/guidellm/profiles/#profile-selection-guide","title":"Profile Selection Guide","text":"Scenario Recommended Profile Why First-time testing <code>sweep</code> Automatically finds safe operating range Load testing <code>constant</code> Predictable, repeatable results Capacity planning <code>throughput</code> Find maximum capacity User simulation <code>concurrent</code> Realistic concurrent load Production-like traffic <code>poisson</code> Natural traffic patterns Baseline latency <code>synchronous</code> Minimum possible latency"},{"location":"adapters/guidellm/profiles/#common-parameters","title":"Common Parameters","text":"<p>All profiles support these common parameters:</p> Parameter Description Default <code>max_seconds</code> Maximum duration in seconds None <code>max_requests</code> Maximum number of requests None <code>max_errors</code> Error threshold before stopping None <code>warmup</code> Warmup period to exclude (% or absolute) None <code>cooldown</code> Cooldown period to exclude (% or absolute) None <p>Warmup Recommendations</p> <p>Use <code>\"warmup\": \"5%\"</code> or <code>\"warmup\": \"10%\"</code> to exclude initial cold-start effects from measurements.</p>"},{"location":"adapters/guidellm/profiles/#examples","title":"Examples","text":""},{"location":"adapters/guidellm/profiles/#quick-test","title":"Quick Test","text":"<p>Fast test with minimal samples:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"constant\",\n    \"rate\": 5,\n    \"max_seconds\": 10,\n    \"max_requests\": 20,\n    \"warmup\": \"0\"\n  }\n}\n</code></pre>"},{"location":"adapters/guidellm/profiles/#production-load-test","title":"Production Load Test","text":"<p>Realistic production simulation:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"poisson\",\n    \"rate\": 50,\n    \"max_seconds\": 300,\n    \"warmup\": \"5%\",\n    \"detect_saturation\": true\n  }\n}\n</code></pre>"},{"location":"adapters/guidellm/profiles/#capacity-test","title":"Capacity Test","text":"<p>Find maximum throughput:</p> <pre><code>{\n  \"benchmark_config\": {\n    \"profile\": \"throughput\",\n    \"max_seconds\": 60,\n    \"max_requests\": 5000,\n    \"warmup\": \"10%\"\n  }\n}\n</code></pre>"},{"location":"adapters/lighteval/","title":"LightEval Adapter","text":""},{"location":"adapters/lighteval/#lighteval-adapter","title":"LightEval Adapter","text":"<p>The LightEval adapter integrates LightEval with the eval-hub evaluation service using the evalhub-sdk framework adapter pattern.</p>"},{"location":"adapters/lighteval/#overview","title":"Overview","text":"<p>LightEval is a lightweight evaluation framework for language models that supports multiple model providers and a wide range of benchmarks.</p>"},{"location":"adapters/lighteval/#key-features","title":"Key Features","text":"<ul> <li>Multiple model providers: Transformers, vLLM, OpenAI, Anthropic, custom endpoints</li> <li>Wide range of benchmarks: HellaSwag, ARC, MMLU, TruthfulQA, GSM8K, and many more</li> <li>Few-shot evaluation: Configurable number of few-shot examples</li> <li>Efficient evaluation: Optimised for speed and resource usage</li> </ul>"},{"location":"adapters/lighteval/#supported-providers","title":"Supported Providers","text":"<ul> <li>transformers: HuggingFace Transformers models</li> <li>vllm: vLLM inference engine</li> <li>openai: OpenAI API</li> <li>anthropic: Anthropic API</li> <li>endpoint: Custom OpenAI-compatible endpoints</li> <li>litellm: LiteLLM proxy</li> </ul>"},{"location":"adapters/lighteval/#architecture","title":"Architecture","text":"<p>The adapter follows the eval-hub framework adapter pattern:</p> <pre><code>graph TD\n    A[ConfigMap&lt;br/&gt;JobSpec] --&gt;|mounted at /meta/job.json| B[Kubernetes Job Pod]\n    B --&gt; C[Adapter Container]\n    B --&gt; D[Sidecar Container]\n\n    C --&gt;|1. Load JobSpec| C\n    C --&gt;|2. Run LightEval| C\n    C --&gt;|3. Parse Results| C\n    C --&gt;|4. Report via callbacks| D\n\n    D --&gt;|Receive status| D\n    D --&gt;|Persist artifacts| D\n    D --&gt;|Store in OCI registry| E[OCI Registry]</code></pre>"},{"location":"adapters/lighteval/#quick-start","title":"Quick Start","text":""},{"location":"adapters/lighteval/#building-the-container","title":"Building the Container","text":"<pre><code>make image-lighteval\n</code></pre>"},{"location":"adapters/lighteval/#running-locally","title":"Running Locally","text":"<pre><code># Set environment for local mode\nexport EVALHUB_MODE=local\nexport EVALHUB_JOB_SPEC_PATH=meta/job.json\nexport SERVICE_URL=http://localhost:8080  # Optional\n\n# Run the adapter\npython main.py\n</code></pre>"},{"location":"adapters/lighteval/#supported-benchmarks","title":"Supported Benchmarks","text":"<p>The adapter supports all LightEval tasks, organised by category:</p>"},{"location":"adapters/lighteval/#commonsense-reasoning","title":"Commonsense Reasoning","text":"<ul> <li>HellaSwag</li> <li>WinoGrande</li> <li>OpenBookQA</li> </ul>"},{"location":"adapters/lighteval/#scientific-reasoning","title":"Scientific Reasoning","text":"<ul> <li>ARC Easy</li> <li>ARC Challenge</li> </ul>"},{"location":"adapters/lighteval/#physical-commonsense","title":"Physical Commonsense","text":"<ul> <li>PIQA</li> </ul>"},{"location":"adapters/lighteval/#truthfulness","title":"Truthfulness","text":"<ul> <li>TruthfulQA (multiple choice)</li> <li>TruthfulQA (generation)</li> </ul>"},{"location":"adapters/lighteval/#mathematics","title":"Mathematics","text":"<ul> <li>GSM8K</li> <li>MATH (various subcategories)</li> </ul>"},{"location":"adapters/lighteval/#knowledge","title":"Knowledge","text":"<ul> <li>MMLU</li> <li>TriviaQA</li> </ul>"},{"location":"adapters/lighteval/#language-understanding","title":"Language Understanding","text":"<ul> <li>GLUE benchmarks</li> </ul>"},{"location":"adapters/lighteval/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration - Detailed configuration reference</li> <li>Benchmarks - Complete benchmark list</li> <li>Examples - Usage examples</li> </ul>"},{"location":"adapters/lighteval/benchmarks/","title":"LightEval Benchmarks","text":""},{"location":"adapters/lighteval/benchmarks/#lighteval-benchmarks","title":"LightEval Benchmarks","text":"<p>Complete list of supported benchmarks.</p>"},{"location":"adapters/lighteval/benchmarks/#coming-soon","title":"Coming Soon","text":"<p>Detailed benchmark documentation is in progress.</p>"},{"location":"adapters/lighteval/benchmarks/#benchmark-categories","title":"Benchmark Categories","text":"<ul> <li>Commonsense Reasoning: HellaSwag, WinoGrande, OpenBookQA</li> <li>Scientific Reasoning: ARC Easy, ARC Challenge</li> <li>Physical Commonsense: PIQA</li> <li>Truthfulness: TruthfulQA</li> <li>Mathematics: GSM8K, MATH</li> <li>Knowledge: MMLU, TriviaQA</li> <li>Language Understanding: GLUE benchmarks</li> </ul> <p>For complete documentation, see the LightEval README.</p>"},{"location":"adapters/lighteval/configuration/","title":"LightEval Configuration","text":""},{"location":"adapters/lighteval/configuration/#lighteval-configuration","title":"LightEval Configuration","text":"<p>Configuration reference for the LightEval adapter.</p>"},{"location":"adapters/lighteval/configuration/#coming-soon","title":"Coming Soon","text":"<p>Detailed configuration documentation is in progress.</p>"},{"location":"adapters/lighteval/configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>{\n  \"job_id\": \"job-123\",\n  \"benchmark_id\": \"hellaswag\",\n  \"model\": {\n    \"name\": \"gpt2\",\n    \"url\": \"http://localhost:8000/v1\"\n  },\n  \"benchmark_config\": {\n    \"provider\": \"endpoint\",\n    \"num_few_shot\": 0,\n    \"random_seed\": 42,\n    \"batch_size\": 1,\n    \"parameters\": {\n      \"temperature\": 0.0,\n      \"max_tokens\": 100\n    }\n  }\n}\n</code></pre> <p>For complete documentation, see the LightEval README.</p>"},{"location":"adapters/lighteval/examples/","title":"LightEval Examples","text":""},{"location":"adapters/lighteval/examples/#lighteval-examples","title":"LightEval Examples","text":"<p>Configuration examples for common scenarios.</p>"},{"location":"adapters/lighteval/examples/#coming-soon","title":"Coming Soon","text":"<p>Detailed examples are in progress.</p>"},{"location":"adapters/lighteval/examples/#single-task","title":"Single Task","text":"<pre><code>{\n  \"job_id\": \"job-123\",\n  \"benchmark_id\": \"hellaswag\",\n  \"model\": {\n    \"name\": \"gpt2\",\n    \"url\": \"http://localhost:8000/v1\"\n  },\n  \"benchmark_config\": {\n    \"provider\": \"endpoint\",\n    \"num_few_shot\": 0\n  }\n}\n</code></pre> <p>For more examples, see the LightEval README.</p>"},{"location":"development/architecture/","title":"Architecture","text":""},{"location":"development/architecture/#architecture","title":"Architecture","text":"<p>Technical architecture of EvalHub adapters.</p>"},{"location":"development/architecture/#adapter-pattern","title":"Adapter Pattern","text":"<p>All adapters implement the <code>FrameworkAdapter</code> interface from evalhub-sdk:</p> <pre><code>from evalhub.adapter import FrameworkAdapter, JobSpec, JobResults, JobCallbacks\nfrom evalhub.adapter.models import JobStatusUpdate, JobStatus, JobPhase, OCIArtifactSpec\n\nclass MyAdapter(FrameworkAdapter):\n    \"\"\"Custom evaluation framework adapter.\"\"\"\n\n    def run_benchmark_job(\n        self,\n        config: JobSpec,\n        callbacks: JobCallbacks\n    ) -&gt; JobResults:\n        \"\"\"\n        Run a benchmark job.\n\n        Args:\n            config: Job specification with configuration\n            callbacks: Callbacks for progress and results reporting\n\n        Returns:\n            JobResults with metrics and metadata\n        \"\"\"\n        # 1. Report initialization\n        callbacks.report_status(JobStatusUpdate(\n            status=JobStatus.RUNNING,\n            phase=JobPhase.INITIALIZING,\n            progress=0.1,\n            message=\"Initializing evaluation\"\n        ))\n\n        # 2. Run evaluation\n        callbacks.report_status(JobStatusUpdate(\n            status=JobStatus.RUNNING,\n            phase=JobPhase.RUNNING_EVALUATION,\n            progress=0.5,\n            message=\"Running evaluation\"\n        ))\n        raw_results = self._run_evaluation(config)\n\n        # 3. Parse results\n        evaluation_results = self._parse_results(raw_results)\n\n        # 4. Persist artifacts to OCI registry\n        oci_result = callbacks.create_oci_artifact(OCIArtifactSpec(\n            files=self._get_output_files(),\n            job_id=config.job_id,\n            benchmark_id=config.benchmark_id,\n            model_name=config.model.name\n        ))\n\n        # 5. Return results\n        return JobResults(\n            job_id=config.job_id,\n            benchmark_id=config.benchmark_id,\n            model_name=config.model.name,\n            results=evaluation_results,\n            overall_score=self._calculate_score(evaluation_results),\n            num_examples_evaluated=len(evaluation_results),\n            duration_seconds=self._get_duration(),\n            oci_artifact=oci_result\n        )\n</code></pre>"},{"location":"development/architecture/#component-diagram","title":"Component Diagram","text":"<pre><code>graph TD\n    A[JobSpec] --&gt;|Input| B[Adapter]\n    B --&gt;|Configuration| C[Framework CLI/SDK]\n    C --&gt;|Raw Results| D[Results Parser]\n    D --&gt;|Metrics| E[JobResults]\n    B --&gt;|Status Updates| F[Callbacks]\n    B --&gt;|Final Results| F\n    F --&gt;|HTTP POST| G[Sidecar]\n    B --&gt;|OCI Artifact Request| F\n    F --&gt;|Direct Push| H[OCIArtifactPersister]\n    H --&gt;|OCI Push| I[Registry]</code></pre>"},{"location":"development/architecture/#data-flow","title":"Data Flow","text":""},{"location":"development/architecture/#1-initialisation","title":"1. Initialisation","text":"<pre><code>sequenceDiagram\n    participant K as Kubernetes\n    participant A as Adapter\n    participant FS as File System\n\n    K-&gt;&gt;A: Mount ConfigMap (/meta/job.json)\n    K-&gt;&gt;A: Set Environment Variables\n    A-&gt;&gt;A: Initialize AdapterSettings from env\n    A-&gt;&gt;FS: Read JobSpec from /meta/job.json\n    FS--&gt;&gt;A: JobSpec data\n    A-&gt;&gt;A: Parse and validate JobSpec\n    A-&gt;&gt;A: Create Callbacks instance</code></pre>"},{"location":"development/architecture/#2-execution","title":"2. Execution","text":"<pre><code>sequenceDiagram\n    participant A as Adapter\n    participant F as Framework\n    participant C as Callbacks\n    participant S as Sidecar\n    participant R as OCI Registry\n\n    A-&gt;&gt;C: report_status(JobStatusUpdate)\n    C-&gt;&gt;S: POST /api/v1/evaluations/jobs/{id}/update\n    A-&gt;&gt;F: Run Benchmark\n    F--&gt;&gt;A: Raw Results\n    A-&gt;&gt;A: Parse Results\n    A-&gt;&gt;C: create_oci_artifact(OCIArtifactSpec)\n    C-&gt;&gt;R: Push OCI Artifact\n    R--&gt;&gt;C: OCIArtifactResult\n    A-&gt;&gt;C: report_results(JobResults)\n    C-&gt;&gt;S: POST /api/v1/evaluations/jobs/{id}/update</code></pre>"},{"location":"development/architecture/#3-artifact-persistence","title":"3. Artifact Persistence","text":"<pre><code>sequenceDiagram\n    participant A as Adapter\n    participant C as Callbacks\n    participant P as OCIArtifactPersister\n    participant R as OCI Registry\n\n    A-&gt;&gt;C: create_oci_artifact(OCIArtifactSpec)\n    C-&gt;&gt;P: persist(spec)\n    P-&gt;&gt;P: Create OCI Layer\n    P-&gt;&gt;R: Push Layer\n    R--&gt;&gt;P: Digest\n    P-&gt;&gt;P: Create Manifest\n    P-&gt;&gt;R: Push Manifest\n    R--&gt;&gt;P: Confirmation\n    P--&gt;&gt;C: OCIArtifactResult\n    C--&gt;&gt;A: OCIArtifactResult</code></pre>"},{"location":"development/architecture/#key-abstractions","title":"Key Abstractions","text":""},{"location":"development/architecture/#jobspec","title":"JobSpec","text":"<p>Job configuration loaded from ConfigMap:</p> <pre><code>@dataclass\nclass JobSpec:\n    job_id: str\n    benchmark_id: str\n    model: ModelConfig\n    benchmark_config: Dict[str, Any]\n    experiment_name: Optional[str] = None\n    tags: Dict[str, str] = field(default_factory=dict)\n    timeout_seconds: int = 60\n</code></pre>"},{"location":"development/architecture/#jobresults","title":"JobResults","text":"<p>Standardised results structure:</p> <pre><code>@dataclass\nclass JobResults:\n    job_id: str\n    benchmark_id: str\n    metrics: Dict[str, Any]\n    overall_score: Optional[float] = None\n    num_examples_evaluated: int = 0\n    duration_seconds: float = 0.0\n    metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"development/architecture/#jobcallbacks","title":"JobCallbacks","text":"<p>Communication with sidecar and OCI registry:</p> <pre><code>class JobCallbacks:\n    def report_status(self, update: JobStatusUpdate) -&gt; None:\n        \"\"\"Report job status update to the service.\n\n        Sends status updates (including progress) to the sidecar which\n        forwards to eval-hub service via POST /api/v1/evaluations/jobs/{id}/update\n        \"\"\"\n\n    def create_oci_artifact(self, spec: OCIArtifactSpec) -&gt; OCIArtifactResult:\n        \"\"\"Create and push OCI artifact directly to registry.\n\n        The SDK persister handles OCI artifact creation and push.\n        Returns artifact digest and reference information.\n        \"\"\"\n\n    def report_results(self, results: JobResults) -&gt; None:\n        \"\"\"Report final evaluation results to the service.\n\n        Sends complete results to sidecar which forwards to eval-hub service\n        via POST /api/v1/evaluations/jobs/{id}/update\n        \"\"\"\n</code></pre>"},{"location":"development/architecture/#configuration-loading","title":"Configuration Loading","text":""},{"location":"development/architecture/#environment-variables","title":"Environment Variables","text":"<pre><code>class AdapterSettings:\n    mode: str = \"k8s\"  # or \"local\"\n    job_spec_path: str = \"/meta/job.json\"\n    service_url: Optional[str] = None\n    registry_url: Optional[str] = None\n    registry_username: Optional[str] = None\n    registry_password: Optional[str] = None\n    registry_insecure: bool = False\n</code></pre>"},{"location":"development/architecture/#jobspec-loading","title":"JobSpec Loading","text":"<pre><code># Automatic loading in k8s mode\nif settings.mode == \"k8s\":\n    with open(\"/meta/job.json\") as f:\n        job_spec = JobSpec(**json.load(f))\n\n# Manual loading in local mode\nelif settings.mode == \"local\":\n    with open(settings.job_spec_path) as f:\n        job_spec = JobSpec(**json.load(f))\n</code></pre>"},{"location":"development/architecture/#error-handling","title":"Error Handling","text":""},{"location":"development/architecture/#framework-errors","title":"Framework Errors","text":"<pre><code>try:\n    results = self._run_evaluation(config)\nexcept FrameworkError as e:\n    callbacks.report_status(JobStatusUpdate(\n        status=JobStatus.FAILED,\n        error_message=str(e)\n    ))\n    raise\n</code></pre>"},{"location":"development/architecture/#timeout-handling","title":"Timeout Handling","text":"<pre><code>@timeout(job_spec.timeout_seconds)\ndef run_benchmark_job(self, job_spec, callbacks):\n    # Evaluation logic\n    pass\n</code></pre>"},{"location":"development/architecture/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code># Continue on partial failures\ntry:\n    artifact_files = self._collect_artifacts()\n    callbacks.create_oci_artifact(OCIArtifactSpec(\n        files=artifact_files,\n        job_id=config.job_id,\n        benchmark_id=config.benchmark_id,\n        model_name=config.model.name\n    ))\nexcept ArtifactError:\n    logger.warning(\"Failed to collect artifacts, continuing\")\n</code></pre>"},{"location":"development/architecture/#best-practices","title":"Best Practices","text":""},{"location":"development/architecture/#1-idempotent-operations","title":"1. Idempotent Operations","text":"<p>Ensure operations can be safely retried:</p> <pre><code>def _run_evaluation(self, config):\n    # Use random seed for reproducibility\n    random.seed(config.random_seed)\n    # Use unique output directories\n    output_dir = f\"/tmp/{job_id}\"\n</code></pre>"},{"location":"development/architecture/#2-progress-reporting","title":"2. Progress Reporting","text":"<p>Report progress at meaningful intervals:</p> <pre><code>total = len(examples)\nfor i, example in enumerate(examples):\n    if i % 10 == 0:\n        progress = i / total\n        callbacks.report_status(JobStatusUpdate(\n            status=JobStatus.RUNNING,\n            phase=JobPhase.RUNNING_EVALUATION,\n            progress=progress,\n            message=f\"Processed {i}/{total}\",\n            completed_steps=i,\n            total_steps=total\n        ))\n</code></pre>"},{"location":"development/architecture/#3-structured-logging","title":"3. Structured Logging","text":"<p>Use structured logging for debugging:</p> <pre><code>logger.info(\n    \"Running evaluation\",\n    extra={\n        \"job_id\": job_spec.job_id,\n        \"benchmark_id\": job_spec.benchmark_id,\n        \"num_examples\": len(examples)\n    }\n)\n</code></pre>"},{"location":"development/architecture/#4-resource-cleanup","title":"4. Resource Cleanup","text":"<p>Clean up temporary resources:</p> <pre><code>try:\n    results = self._run_evaluation(config)\nfinally:\n    self._cleanup_temp_files()\n</code></pre>"},{"location":"development/architecture/#testing","title":"Testing","text":""},{"location":"development/architecture/#unit-tests","title":"Unit Tests","text":"<p>Test individual components:</p> <pre><code>def test_parse_results():\n    adapter = MyAdapter(settings)\n    raw_results = {\"accuracy\": 0.85}\n    metrics = adapter._parse_results(raw_results)\n    assert metrics[\"accuracy\"] == 0.85\n</code></pre>"},{"location":"development/architecture/#integration-tests","title":"Integration Tests","text":"<p>Test with mock service:</p> <pre><code>@pytest.fixture\ndef mock_service():\n    with MockEvalHubService() as service:\n        yield service\n\ndef test_full_workflow(mock_service):\n    adapter = MyAdapter(settings)\n    results = adapter.run_benchmark_job(job_spec, callbacks)\n    assert results.job_id == job_spec.job_id\n</code></pre>"},{"location":"development/creating-adapters/","title":"Creating Adapters","text":""},{"location":"development/creating-adapters/#creating-adapters","title":"Creating Adapters","text":"<p>Guide for creating new eval-hub adapters.</p>"},{"location":"development/creating-adapters/#coming-soon","title":"Coming Soon","text":"<p>Detailed adapter creation guide is in progress.</p>"},{"location":"development/creating-adapters/#quick-template","title":"Quick Template","text":"<pre><code>from evalhub.adapter import FrameworkAdapter, JobSpec, JobResults, JobCallbacks\n\nclass MyAdapter(FrameworkAdapter):\n    def run_benchmark_job(\n        self,\n        job_spec: JobSpec,\n        callbacks: JobCallbacks\n    ) -&gt; JobResults:\n        # Your implementation here\n        pass\n</code></pre> <p>For complete examples, see existing adapters:</p> <ul> <li>GuideLLM Adapter</li> <li>LightEval Adapter</li> </ul>"},{"location":"development/openshift-setup/","title":"OpenShift Setup","text":""},{"location":"development/openshift-setup/#openshift-development-setup","title":"OpenShift Development Setup","text":"<p>Complete guide for deploying and developing EvalHub on OpenShift with OpenDataHub.</p>"},{"location":"development/openshift-setup/#overview","title":"Overview","text":"<p>This guide covers deploying a custom EvalHub instance on OpenShift integrated with OpenDataHub.</p> <p>OpenDataHub Integration</p> <p>EvalHub is integrated into OpenDataHub via the TrustyAI Operator, which manages EvalHub deployments as custom resources.</p>"},{"location":"development/openshift-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"development/openshift-setup/#required-tools","title":"Required Tools","text":"<ul> <li>OpenShift CLI (<code>oc</code>) - Version 4.12+</li> <li>kubectl - Compatible with your OpenShift version</li> <li>jq - JSON processor for manifest manipulation</li> <li>Git - For cloning repositories</li> <li>Podman or Docker - For building custom images</li> </ul>"},{"location":"development/openshift-setup/#cluster-access","title":"Cluster Access","text":"<p>Ensure you have access to an OpenShift cluster:</p> <pre><code># Login to OpenShift\noc login --server=https://api.your-cluster.example.com:6443\n\n# Verify access\noc whoami\noc cluster-info\n</code></pre> <p>Administrator Access Required</p> <p>Installing OpenDataHub and the TrustyAI Operator requires cluster-admin or equivalent permissions.</p>"},{"location":"development/openshift-setup/#opendatahub-installation","title":"OpenDataHub Installation","text":"<p>OpenDataHub provides the foundation for deploying EvalHub on OpenShift.</p>"},{"location":"development/openshift-setup/#1-install-opendatahub-operator","title":"1. Install OpenDataHub Operator","text":"<p>Install the OpenDataHub Operator (version 3.3 or higher) from OperatorHub in the OpenShift web console:</p> <ol> <li>Navigate to Operators \u2192 OperatorHub</li> <li>Search for \"OpenDataHub\"</li> <li>Click Install</li> <li>Select fast channel</li> <li>Choose All namespaces on the cluster installation mode</li> <li>Click Install</li> </ol> <p>Wait for the operator installation to complete:</p> <pre><code># Check operator installation\noc get csv -n openshift-operators | grep opendatahub\n\n# Should show PHASE: \"Succeeded\"\n</code></pre>"},{"location":"development/openshift-setup/#2-configure-opendatahub","title":"2. Configure OpenDataHub","text":"<p>After the operator is installed, configure OpenDataHub from the operator's dashboard:</p> <ol> <li>Navigate to Operators \u2192 Installed Operators</li> <li>Select OpenDataHub Operator</li> <li>Go to the DSCInitialization tab</li> <li>Click Create DSCInitialization</li> <li>Review or just use default settings</li> <li>Click Create</li> <li>Await \"Phase: Ready\"</li> <li>Go to the Data Science Cluster tab</li> <li>Click Create DataScienceCluster</li> <li>Review or just use default settings for most components, do not save/create yet</li> <li>Important: Under the TrustyAI component, configure LMEval security settings:</li> <li>Set <code>permitCodeExecution</code> to allow</li> <li>Set <code>permitOnline</code> to allow</li> <li>Click Create</li> </ol> <p>Security Settings</p> <p>The <code>permitCodeExecution</code> and <code>permitOnline</code> settings control whether evaluation jobs can execute arbitrary code or access the internet. For development and testing, set these to <code>allow</code>. For production deployments, consider setting to <code>deny</code> based on your security requirements.</p> <p>Verify the DataScienceCluster is ready:</p> <pre><code># Check DSC status\noc get datasciencecluster default-dsc -o jsonpath='{.status.phase}'\n\n# Should output: \"Ready\"\n\n# List deployed components\noc get pods -n opendatahub\n</code></pre>"},{"location":"development/openshift-setup/#trustyai-operator-deployment","title":"TrustyAI Operator Deployment","text":"<p>The TrustyAI Operator manages EvalHub custom resources.</p>"},{"location":"development/openshift-setup/#1-verify-trustyai-operator-installation","title":"1. Verify TrustyAI Operator Installation","text":"<p>The TrustyAI Operator is installed automatically as part of the DataScienceCluster:</p> <pre><code># Check TrustyAI Operator pods\noc get pods -n opendatahub -l app.kubernetes.io/part-of=trustyai\n\n# Check TrustyAI CRDs\noc get crd | grep trustyai\n</code></pre> <p>Example CRDs (actual list may vary):</p> <ul> <li><code>evalhubs.trustyai.opendatahub.io</code> - EvalHub instances</li> <li><code>trustyaiservices.trustyai.opendatahub.io</code> - TrustyAI services</li> <li><code>lmevaljobrequests.trustyai.opendatahub.io</code> - LMEval job requests</li> </ul>"},{"location":"development/openshift-setup/#evalhub-custom-resource","title":"EvalHub Custom Resource","text":"<p>Deploy an EvalHub instance using a custom resource.</p>"},{"location":"development/openshift-setup/#1-create-evalhub-namespace","title":"1. Create EvalHub Namespace","text":"<pre><code># Create namespace for EvalHub workloads\noc create namespace evalhub-test\n\n# Label for monitoring and networking\noc label namespace evalhub-test \\\n  opendatahub.io/dashboard=true \\\n  evalhub.trustyai.io/managed=true\n</code></pre>"},{"location":"development/openshift-setup/#2-deploy-evalhub-instance","title":"2. Deploy EvalHub Instance","text":"<p>Create an EvalHub custom resource:</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: EvalHub\nmetadata:\n  name: evalhub\n  namespace: evalhub-test\nspec:\n  replicas: 1\nEOF\n</code></pre>"},{"location":"development/openshift-setup/#3-verify-deployment","title":"3. Verify Deployment","text":"<pre><code># Check EvalHub CR status\noc get evalhub evalhub -n evalhub-test\n\n# Should show STATUS: Ready\n\n# Check EvalHub pods\noc get pods -n evalhub-test -l app=eval-hub\n\n# Check EvalHub service\noc get svc -n evalhub-test -l app=eval-hub\n\n# Check route (if OAuth is enabled)\noc get route -n evalhub-test -l app=eval-hub\n</code></pre>"},{"location":"development/openshift-setup/#4-access-evalhub","title":"4. Access EvalHub","text":"<p>Get the EvalHub URL:</p> <pre><code># Get route URL\nEVALHUB_URL=$(oc get route evalhub -n evalhub-test -o jsonpath='{.spec.host}')\necho \"EvalHub URL: https://$EVALHUB_URL\"\n\n# Test health endpoint\ncurl -k \"https://$EVALHUB_URL/api/v1/health\"\n</code></pre> <p>With OAuth enabled:</p> <pre><code># Get authentication token\nTOKEN=$(oc whoami -t)\n\n# Access EvalHub with token\ncurl -k -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://$EVALHUB_URL/api/v1/evaluations/providers\"\n</code></pre>"},{"location":"development/openshift-setup/#development-workflow","title":"Development Workflow","text":"<p>Develop and test custom EvalHub providers on OpenShift.</p>"},{"location":"development/openshift-setup/#1-clone-repositories","title":"1. Clone Repositories","text":"<pre><code># Clone EvalHub repositories\ngit clone https://github.com/eval-hub/eval-hub.git\ngit clone https://github.com/eval-hub/eval-hub-sdk.git\ngit clone https://github.com/eval-hub/eval-hub-contrib.git\n\n# Clone TrustyAI Operator (for operator development)\ngit clone https://github.com/trustyai-explainability/trustyai-service-operator.git\n</code></pre>"},{"location":"development/openshift-setup/#2-upload-custom-operator-manifests","title":"2. Upload Custom Operator Manifests","text":"<p>You can deploy EvalHub directly using the manifests from the cloned TrustyAI operator repository without any modifications. Alternatively, if you want to use custom images or configurations, you can modify the manifests before uploading them.</p> <p>For development and testing, you can upload TrustyAI operator manifests directly to the OpenDataHub operator without rebuilding operator images. This allows you to iterate quickly on operator configurations, CRDs, and RBAC definitions.</p> <p>Development Only</p> <p>This approach is intended for development and testing only. Production deployments should use proper operator image releases.</p>"},{"location":"development/openshift-setup/#overview_1","title":"Overview","text":"<p>The process involves:</p> <ol> <li>Creating a PersistentVolumeClaim to store custom manifests</li> <li>Patching the OpenDataHub operator CSV to mount the PVC</li> <li>Copying your custom manifests into the operator pod</li> <li>Restarting the operators to load the new manifests</li> </ol> <p>This is based on the OpenDataHub component development workflow.</p>"},{"location":"development/openshift-setup/#prepare-custom-manifests","title":"Prepare Custom Manifests","text":"<p>Clone the TrustyAI operator repository to get the manifests:</p> <pre><code># Clone TrustyAI operator (if not already cloned)\ngit clone https://github.com/trustyai-explainability/trustyai-service-operator.git\ncd trustyai-service-operator\n</code></pre> <p>The operator repository has the following structure under <code>config/</code>:</p> <pre><code>config/\n\u251c\u2500\u2500 crd/\n\u2502   \u2514\u2500\u2500 bases/\n\u2502       \u251c\u2500\u2500 trustyai.opendatahub.io_evalhubs.yaml\n\u2502       \u251c\u2500\u2500 trustyai.opendatahub.io_lmevaljobrequests.yaml\n\u2502       \u2514\u2500\u2500 trustyai.opendatahub.io_trustyaiservices.yaml\n\u251c\u2500\u2500 default/\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 manager_auth_proxy_patch.yaml\n\u251c\u2500\u2500 manager/\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 manager.yaml\n\u251c\u2500\u2500 manifests/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 overlays/\n\u2502   \u2514\u2500\u2500 odh/\n\u2502       \u251c\u2500\u2500 kustomization.yaml\n\u2502       \u2514\u2500\u2500 params.env\n\u251c\u2500\u2500 rbac/\n\u2502   \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml\n\u2502   \u251c\u2500\u2500 auth_proxy_role.yaml\n\u2502   \u251c\u2500\u2500 auth_proxy_role_binding.yaml\n\u2502   \u251c\u2500\u2500 auth_proxy_service.yaml\n\u2502   \u251c\u2500\u2500 evalhub_jobs_proxy_role.yaml\n\u2502   \u251c\u2500\u2500 evalhub_resource_manager_binding.yaml\n\u2502   \u251c\u2500\u2500 evalhub_resource_manager_role.yaml\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 leader_election_role.yaml\n\u2502   \u251c\u2500\u2500 leader_election_role_binding.yaml\n\u2502   \u251c\u2500\u2500 role.yaml\n\u2502   \u251c\u2500\u2500 role_binding.yaml\n\u2502   \u251c\u2500\u2500 service_account.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 samples/\n    \u2514\u2500\u2500 ...\n</code></pre> <p>To deploy as-is, use the manifests without modifications. To customize, edit the configuration files:</p> <pre><code># Example: Update image references\nvim config/overlays/odh/params.env\n\n# Example: Modify RBAC permissions\nvim config/rbac/evalhub_resource_manager_role.yaml\n</code></pre> <p>Info</p> <p>In the remainder of this document, references to <code>./config</code> in shell commands refer to the <code>config/</code> directory within the <code>trustyai-service-operator</code> repository cloned above.</p>"},{"location":"development/openshift-setup/#upload-manifests","title":"Upload Manifests","text":"<p>Follow these steps to upload your custom manifests to the OpenDataHub operator:</p> <p>Step 1: Create PersistentVolumeClaim</p> <pre><code>cat &lt;&lt;EOF | oc apply -n openshift-operators -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: trustyai-manifests\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre> <p>Step 2: Patch CSV to mount manifests</p> <pre><code># Get the OpenDataHub operator CSV name\nCSV=$(oc get csv -n openshift-operators -o name | grep opendatahub-operator | head -n1 | cut -d/ -f2)\n\n# Patch CSV to mount the PVC\noc patch csv \"${CSV}\" -n openshift-operators --type json -p '[\n  {\"op\": \"replace\", \"path\": \"/spec/install/spec/deployments/0/spec/replicas\", \"value\": 1},\n  {\"op\": \"replace\", \"path\": \"/spec/install/spec/deployments/0/spec/strategy/type\", \"value\": \"Recreate\"},\n  {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/securityContext\", \"value\": {\"fsGroup\": 1001}},\n  {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/volumes/-\", \"value\": {\"name\": \"trustyai-manifests\", \"persistentVolumeClaim\": {\"claimName\": \"trustyai-manifests\"}}},\n  {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/volumeMounts/-\", \"value\": {\"name\": \"trustyai-manifests\", \"mountPath\": \"/opt/manifests/trustyai\"}}\n]'\n</code></pre> <p>CSV Patch Idempotency</p> <p>The patch command may fail if already applied. This is expected and can be safely ignored.</p> <p>Step 3: Copy manifests into operator pod</p> <pre><code># Wait for the operator pod to be ready after patching\noc wait --for=condition=ready pod -n openshift-operators -l name=opendatahub-operator --timeout=120s\n\n# Get the operator pod name\nPOD=$(oc get pod -n openshift-operators -l name=opendatahub-operator -o jsonpath='{.items[0].metadata.name}')\n\n# Copy manifests from your local repository to the operator pod\noc cp ./config/. openshift-operators/${POD}:/opt/manifests/trustyai\n</code></pre> <p>Step 4: Restart operators</p> <pre><code># Restart OpenDataHub operator\noc rollout restart deploy -n openshift-operators -l name=opendatahub-operator\noc rollout status deploy -n openshift-operators -l name=opendatahub-operator\n\n# Restart TrustyAI operator\noc rollout restart deployment/trustyai-service-operator-controller-manager -n opendatahub\n</code></pre> <p>Wait for the operators to be ready:</p> <pre><code># Wait for OpenDataHub operator\noc wait --for=condition=available deployment -n openshift-operators -l name=opendatahub-operator --timeout=120s\n\n# Wait for TrustyAI operator\noc wait --for=condition=available deployment/trustyai-service-operator-controller-manager -n opendatahub --timeout=120s\n</code></pre>"},{"location":"development/openshift-setup/#verify-custom-manifests","title":"Verify Custom Manifests","text":"<p>After uploading and restarting, verify the operators are using your custom manifests:</p> <pre><code># Check operator pods are running\noc get pods -n openshift-operators -l name=opendatahub-operator\noc get pods -n opendatahub -l  app.kubernetes.io/part-of=trustyai\n\n# Check TrustyAI operator logs for manifest loading\noc logs -n opendatahub -l  app.kubernetes.io/part-of=trustyai --tail=50\n\n# Verify ConfigMap has your custom values\noc get configmap trustyai-service-operator-config -n opendatahub -o yaml\n</code></pre> <p>Test your changes by creating or updating an EvalHub CR:</p> <pre><code># Delete existing EvalHub if it exists\noc delete evalhub evalhub -n evalhub-test --ignore-not-found\n\n# Create EvalHub with custom manifests\noc apply -f - &lt;&lt;EOF\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: EvalHub\nmetadata:\n  name: evalhub\n  namespace: evalhub-test\nspec:\n  replicas: 1\nEOF\n\n# Check if custom changes are applied\noc get evalhub evalhub -n evalhub-test -o yaml\noc get pods -n evalhub-test -l app=eval-hub\n</code></pre>"},{"location":"development/openshift-setup/#iterate-on-changes","title":"Iterate on Changes","text":"<p>To update your manifests during development:</p> <ol> <li>Modify manifests in your local directory (e.g., <code>./trustyai-service-operator/config/</code>)</li> <li>Copy updated manifests to the operator pod:    <pre><code>POD=$(oc get pod -n openshift-operators -l name=opendatahub-operator -o jsonpath='{.items[0].metadata.name}')\noc cp ./config/. openshift-operators/${POD}:/opt/manifests/trustyai\n</code></pre></li> <li>Restart operators:    <pre><code>oc rollout restart deploy -n openshift-operators -l name=opendatahub-operator\noc rollout status deploy -n openshift-operators -l name=opendatahub-operator\noc rollout restart deployment/trustyai-service-operator-controller-manager -n opendatahub\n</code></pre></li> <li>Test changes by recreating the EvalHub CR</li> </ol> <p>The operators will automatically load your updated manifests.</p>"},{"location":"development/openshift-setup/#important-notes","title":"Important Notes","text":"<ul> <li>RWO Storage: The PVC uses ReadWriteOnce access mode, requiring single-replica operator deployment</li> <li>Recreate Strategy: The CSV is patched to use \"Recreate\" deployment strategy to avoid PVC conflicts</li> <li>Not Idempotent: The CSV patch may fail if already applied (this is expected)</li> <li>Development Only: This workflow is not suitable for production deployments</li> </ul>"},{"location":"development/openshift-setup/#cleanup","title":"Cleanup","text":"<p>To remove custom manifests and restore the operator to default:</p> <pre><code># Delete the PVC\noc delete pvc trustyai-manifests -n openshift-operators\n\n# Restore CSV to default by reinstalling the operator\n# Or manually remove the volumeMounts and volumes from the CSV\n</code></pre>"},{"location":"development/openshift-setup/#3-build-custom-evalhub-image","title":"3. Build Custom EvalHub Image","text":"<p>If you want to use a custom EvalHub server image (for example, with modified code or dependencies), build and push it to your container registry, then update the operator manifests before uploading them.</p> <p>Login to Quay if you haven't done already:</p> <pre><code>podman login quay.io\n</code></pre> <p>Build a custom EvalHub server image:</p> <pre><code>cd eval-hub\n\n# Build with Podman\npodman build --platform linux/amd64 -t quay.io/your-org/eval-hub:dev .\n\n# Push to registry\npodman push quay.io/your-org/eval-hub:dev\n</code></pre> <p>Update the operator manifests to use your custom image:</p> <pre><code># Edit the params.env file in the operator repository\ncd trustyai-service-operator\nvim config/overlays/odh/params.env\n\n# Change evalHubImage to your custom image\n# evalHubImage=quay.io/your-org/eval-hub:dev\n</code></pre> <p>Then upload the modified manifests using the commands from the previous section:</p> <pre><code># Copy manifests to operator pod\nPOD=$(oc get pod -n openshift-operators -l name=opendatahub-operator -o jsonpath='{.items[0].metadata.name}')\noc cp ./config/. openshift-operators/${POD}:/opt/manifests/trustyai\n\n# Restart operators\noc rollout restart deploy -n openshift-operators -l name=opendatahub-operator\noc rollout status deploy -n openshift-operators -l name=opendatahub-operator\noc rollout restart deployment/trustyai-service-operator-controller-manager -n opendatahub\n</code></pre> <p>Delete and recreate the EvalHub instance to use the new image:</p> <pre><code># Delete existing instance\noc delete evalhub evalhub -n evalhub-test\n\n# Recreate with new image\noc apply -f - &lt;&lt;EOF\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: EvalHub\nmetadata:\n  name: evalhub\n  namespace: evalhub-test\nspec:\n  replicas: 1\nEOF\n</code></pre>"},{"location":"development/openshift-setup/#4-test-deployment","title":"4. Test Deployment","text":"<p>Verify the EvalHub deployment by listing available providers and submitting a test evaluation.</p> <p>List available providers:</p> <pre><code># Get EvalHub URL and token\nEVALHUB_URL=$(oc get route evalhub -n evalhub-test -o jsonpath='{.spec.host}')\nTOKEN=$(oc whoami -t)\n\n# List all providers\ncurl -k -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://$EVALHUB_URL/api/v1/evaluations/providers\" | jq .\n\n# List benchmarks for a specific provider\ncurl -k -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://$EVALHUB_URL/api/v1/evaluations/providers/lm_evaluation_harness/benchmarks\" | jq .\n</code></pre> <p>Submit a test evaluation:</p> <pre><code># Create evaluation request\ncat &gt; eval-request.json &lt;&lt;EOF\n{\n  \"model\": {\n    \"url\": \"http://vllm-server.models.svc.cluster.local:8000/v1\",\n    \"name\": \"meta-llama/Llama-3.2-1B-Instruct\"\n  },\n  \"benchmarks\": [\n    {\n      \"id\": \"mmlu\",\n      \"provider_id\": \"lm_evaluation_harness\"\n    }\n  ],\n  \"experiment\": {\n    \"name\": \"test-deployment\",\n    \"tags\": [\n      {\n        \"key\": \"environment\",\n        \"value\": \"development\"\n      }\n    ]\n  }\n}\nEOF\n\n# Submit evaluation\ncurl -k -X POST \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @eval-request.json \\\n  \"https://$EVALHUB_URL/api/v1/evaluations/jobs\" | jq .\n</code></pre>"},{"location":"development/openshift-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/openshift-setup/#evalhub-pod-not-starting","title":"EvalHub Pod Not Starting","text":"<p>Symptoms: EvalHub pod stuck in Pending or CrashLoopBackOff</p> <p>Diagnostics:</p> <pre><code># Check pod status\noc get pods -n evalhub-test -l app=eval-hub\n\n# Describe pod for events\noc describe pod -n evalhub-test -l app=eval-hub\n\n# Check logs\noc logs -n evalhub-test -l app=eval-hub --tail=100\n</code></pre> <p>Common causes:</p> <ol> <li> <p>Image pull failure </p><pre><code># Check image pull secrets\noc get secret -n evalhub-test\n\n# Verify image exists\npodman pull quay.io/opendatahub/odh-eval-hub:latest\n</code></pre><p></p> </li> <li> <p>Insufficient resources </p><pre><code># Check node capacity\noc describe node | grep -A5 \"Allocated resources\"\n\n# Reduce resource requests in CR\noc edit evalhub evalhub -n evalhub-test\n</code></pre><p></p> </li> <li> <p>ConfigMap missing </p><pre><code># Check operator ConfigMap\noc get configmap trustyai-service-operator-config -n opendatahub\n</code></pre><p></p> </li> </ol>"},{"location":"development/openshift-setup/#evaluation-jobs-failing","title":"Evaluation Jobs Failing","text":"<p>Symptoms: Jobs complete but report failure status</p> <p>Diagnostics:</p> <pre><code># Find evaluation job\noc get jobs -n evalhub-test -l app=eval-hub\n\n# Check job status\noc describe job &lt;job-name&gt; -n evalhub-test\n\n# Check adapter pod logs\noc logs -n evalhub-test -l job-name=&lt;job-name&gt; -c adapter\n\n# Check sidecar logs (if present)\noc logs -n evalhub-test -l job-name=&lt;job-name&gt; -c sidecar\n</code></pre> <p>Common causes:</p> <ol> <li> <p>Callback URL unreachable </p><pre><code># Verify EvalHub service is accessible from job pods\noc get svc evalhub -n evalhub-test\n\n# Test connectivity from job pod\noc exec -it &lt;job-pod&gt; -n evalhub-test -- \\\n  curl -v http://evalhub.evalhub-test.svc.cluster.local:8080/api/v1/health\n</code></pre><p></p> </li> <li> <p>Model endpoint unreachable </p><pre><code># Check if model server is accessible\noc exec -it &lt;job-pod&gt; -n evalhub-test -- \\\n  curl -v http://vllm-server.models.svc.cluster.local:8000/v1/models\n</code></pre><p></p> </li> <li> <p>Insufficient job resources </p><pre><code># Check for OOMKilled status\noc get pods -n evalhub-test -l job-name=&lt;job-name&gt; \\\n  -o jsonpath='{.items[*].status.containerStatuses[*].state}'\n\n# Increase memory limits in provider config\n</code></pre><p></p> </li> </ol>"},{"location":"development/openshift-setup/#oauth-authentication-issues","title":"OAuth Authentication Issues","text":"<p>Symptoms: 401 Unauthorized when accessing EvalHub API</p> <p>Diagnostics:</p> <pre><code># Check OAuth configuration\noc get route evalhub -n evalhub-test -o yaml | grep -A10 tls\n\n# Verify token is valid\noc whoami -t\n\n# Test authentication\nTOKEN=$(oc whoami -t)\ncurl -k -v -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://$(oc get route evalhub -n evalhub-test -o jsonpath='{.spec.host}')/api/v1/health\"\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Regenerate token </p><pre><code>oc login --token=&lt;new-token&gt;\n</code></pre><p></p> </li> <li> <p>Check RBAC configuration </p><pre><code># Verify ClusterRoleBinding exists\noc get clusterrolebinding | grep evalhub\n\n# Check user permissions\noc auth can-i create evaluationjobs.trustyai.opendatahub.io\n</code></pre><p></p> </li> </ol>"},{"location":"development/openshift-setup/#operator-not-reconciling","title":"Operator Not Reconciling","text":"<p>Symptoms: EvalHub CR created but no pods deployed</p> <p>Diagnostics:</p> <pre><code># Check operator logs\noc logs -n opendatahub \\\n  -l  app.kubernetes.io/part-of=trustyai \\\n  --tail=100\n\n# Check EvalHub CR status\noc get evalhub evalhub -n evalhub-test -o yaml\n\n# Check events\noc get events -n evalhub-test --sort-by='.lastTimestamp'\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Restart operator </p><pre><code>oc delete pod -n opendatahub \\\n  -l  app.kubernetes.io/part-of=trustyai\n</code></pre><p></p> </li> <li> <p>Check CRD versions </p><pre><code># Verify CRD is installed\noc get crd evalhubs.trustyai.opendatahub.io\n\n# Check stored versions\noc get crd evalhubs.trustyai.opendatahub.io \\\n  -o jsonpath='{.status.storedVersions}'\n</code></pre><p></p> </li> <li> <p>Recreate EvalHub CR </p><pre><code>oc delete evalhub evalhub -n evalhub-test\noc apply -f evalhub-cr.yaml\n</code></pre><p></p> </li> </ol>"},{"location":"development/openshift-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Creating Adapters - Build custom evaluation providers</li> <li>Architecture - Understand EvalHub architecture</li> <li>API Reference - REST API documentation</li> <li>TrustyAI Operator Documentation - Operator details</li> </ul>"},{"location":"development/openshift-setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>OpenDataHub Documentation</li> <li>OpenShift Documentation</li> <li>Kubernetes Operators</li> <li>EvalHub GitHub</li> <li>TrustyAI Operator GitHub</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Install EvalHub components for your use case.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Python 3.12+</li> </ul>"},{"location":"getting-started/installation/#optional-for-production-deployment","title":"Optional (for production deployment)","text":"<ul> <li>Kubernetes/OpenShift cluster</li> <li>TrustyAI Operator</li> </ul>"},{"location":"getting-started/installation/#server-installation","title":"Server Installation","text":"<p>The EvalHub server orchestrates evaluation jobs and manages providers.</p> Kubernetes/OpenShiftLocal Development <p>Install using the TrustyAI Operator:</p> <pre><code># Install TrustyAI Operator\nkubectl apply -f https://github.com/trustyai-explainability/trustyai-service-operator/releases/latest/download/trustyai-operator.yaml\n\n# Create EvalHub instance\nkubectl apply -f - &lt;&lt;EOF\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: EvalHub\nmetadata:\n  name: evalhub\n  namespace: evalhub\nspec:\n  replicas: 1\nEOF\n</code></pre> <p>For local development, the SDK client automatically manages the server:</p> <pre><code>pip install eval-hub-sdk[server]\n</code></pre> <p>When you use the Python SDK client, it will transparently start and manage the local server on <code>http://localhost:8080</code> using SQLite for storage. No manual server startup required.</p>"},{"location":"getting-started/installation/#client-installation","title":"Client Installation","text":"<p>The client SDK allows you to submit evaluations and query results from Python.</p> <pre><code>pip install eval-hub-sdk[client]\n</code></pre>"},{"location":"getting-started/installation/#usage","title":"Usage","text":"<pre><code>from evalhub.client import EvalHubClient\nfrom evalhub.models.api import ModelConfig, BenchmarkSpec\n\n# Connect to EvalHub server\nclient = EvalHubClient(base_url=\"http://localhost:8080\")\n\n# List available providers\nproviders = client.list_providers()\n\n# Submit evaluation\njob = client.submit_evaluation(\n    model=ModelConfig(\n        url=\"http://localhost:11434/v1\",\n        name=\"qwen2.5:1.5b\"\n    ),\n    benchmarks=[\n        BenchmarkSpec(\n            benchmark_id=\"mmlu\",\n            provider_id=\"lm_evaluation_harness\"\n        )\n    ]\n)\n\n# Check job status\nstatus = client.get_job_status(job.job_id)\nprint(f\"Status: {status.status}\")\n</code></pre>"},{"location":"getting-started/installation/#provider-configuration","title":"Provider Configuration","text":"<p>Providers are evaluation frameworks (LightEval, GuideLLM, RAGAS, etc.) that run as containerised adapters.</p>"},{"location":"getting-started/installation/#adding-a-provider","title":"Adding a Provider","text":"Kubernetes/OpenShiftLocal Development <p>Create a ConfigMap with the provider configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: evalhub-providers\n  namespace: evalhub\ndata:\n  providers.yaml: |\n    providers:\n    - provider_id: guidellm\n      provider_type: performance\n      provider_name: GuideLLM\n      description: Performance benchmarking framework\n      container_image: quay.io/eval-hub/community-guidellm:latest\n      benchmarks:\n      - benchmark_id: performance_test\n        name: Performance Benchmark\n        description: Measure throughput and latency\n        category: performance\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f evalhub-providers.yaml\n</code></pre> <p>Create a <code>providers.yaml</code> file:</p> <pre><code>providers:\n- provider_id: guidellm\n  provider_type: performance\n  provider_name: GuideLLM\n  description: Performance benchmarking framework\n  container_image: quay.io/eval-hub/community-guidellm:latest\n  benchmarks:\n  - benchmark_id: performance_test\n    name: Performance Benchmark\n    description: Measure throughput and latency\n    category: performance\n</code></pre> <p>Place it in the server's configuration directory or specify its path via environment variable:</p> <pre><code>export EVALHUB_PROVIDERS_CONFIG=./providers.yaml\neval-hub-server\n</code></pre>"},{"location":"getting-started/installation/#using-the-provider","title":"Using the Provider","text":"<p>Once the provider is configured, it can be used like any built-in provider:</p> <pre><code># List all providers (including custom ones)\nproviders = client.list_providers()\n\n# Submit evaluation using custom provider\njob = client.submit_evaluation(\n    model=ModelConfig(\n        url=\"http://vllm-server:8000/v1\",\n        name=\"meta-llama/Llama-3.2-1B-Instruct\"\n    ),\n    benchmarks=[\n        BenchmarkSpec(\n            benchmark_id=\"performance_test\",\n            provider_id=\"guidellm\",\n            config={\n                \"profile\": \"constant\",\n                \"rate\": 10,\n                \"max_seconds\": 60\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"getting-started/installation/#model-serving-optional","title":"Model Serving (Optional)","text":"<p>For testing evaluations, you'll need a model serving endpoint.</p> vLLM (OpenShift)Ollama (Local) <p>Deploy vLLM on OpenShift:</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-server\n  namespace: evalhub\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm\n  template:\n    metadata:\n      labels:\n        app: vllm\n    spec:\n      containers:\n      - name: vllm\n        image: vllm/vllm-openai:latest\n        args:\n        - --model\n        - meta-llama/Llama-3.2-1B-Instruct\n        - --port\n        - \"8000\"\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-server\n  namespace: evalhub\nspec:\n  selector:\n    app: vllm\n  ports:\n  - port: 8000\n    targetPort: 8000\nEOF\n</code></pre> <p>Install and run Ollama for local development:</p> <pre><code># Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull and run a model\nollama run qwen2.5:1.5b\n\n# Ollama serves at http://localhost:11434/v1\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":""},{"location":"getting-started/installation/#server","title":"Server","text":"<p>Check the server is running:</p> <pre><code># Local\ncurl http://localhost:8080/api/v1/health\n\n# Kubernetes\nkubectl get pods -n evalhub -l app=evalhub-server\n</code></pre>"},{"location":"getting-started/installation/#client","title":"Client","text":"<p>Verify client installation:</p> <pre><code>from evalhub.client import EvalHubClient\nprint('Client installed successfully')\n</code></pre>"},{"location":"getting-started/installation/#provider","title":"Provider","text":"<p>List available providers:</p> <pre><code>curl http://localhost:8080/api/v1/providers\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Run your first evaluation</li> <li>Server Configuration - Configure the server</li> <li>API Reference - REST API documentation</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#overview","title":"Overview","text":"<p>EvalHub is an open source evaluation orchestration platform for Large Language Models, consisting of three integrated components.</p>"},{"location":"getting-started/overview/#the-evalhub-ecosystem","title":"The EvalHub Ecosystem","text":""},{"location":"getting-started/overview/#evalhub-server","title":"EvalHub Server","text":"<p>REST API orchestration service that manages evaluation workflows.</p> <p>Key responsibilities:</p> <ul> <li>Expose versioned REST API (v1) for evaluation submission</li> <li>Orchestrate Kubernetes Job creation and lifecycle management</li> <li>Manage provider registry and benchmark discovery</li> <li>Store job metadata and results in PostgreSQL</li> <li>Export Prometheus metrics for observability</li> </ul> <p>Primary deployment targets:</p> <ul> <li>Local development (http://localhost:8080)</li> <li>OpenShift clusters (production)</li> <li>Generic Kubernetes clusters</li> </ul>"},{"location":"getting-started/overview/#evalhub-sdk","title":"EvalHub SDK","text":"<p>Python SDK providing three focused packages for different use cases.</p> <p>evalhub.client - Submit evaluations to the service: </p><pre><code>from evalhub.client import EvalHubClient\n\nclient = EvalHubClient(base_url=\"http://localhost:8080\")\njob = client.submit_evaluation(model=..., benchmarks=[...])\n</code></pre><p></p> <p>evalhub.adapter - Build framework adapters: </p><pre><code>from evalhub.adapter import FrameworkAdapter, JobSpec, JobResults\n\nclass MyAdapter(FrameworkAdapter):\n    def run_benchmark_job(self, job_spec, callbacks) -&gt; JobResults:\n        # Framework integration logic\n        pass\n</code></pre><p></p> <p>evalhub.models - Shared data structures: </p><pre><code>from evalhub.models.api import ModelConfig, BenchmarkSpec\n</code></pre><p></p>"},{"location":"getting-started/overview/#evalhub-contrib","title":"EvalHub Contrib","text":"<p>Community-contributed framework adapters packaged as container images.</p> <p>Available adapters:</p> <ul> <li>LightEval: Language model evaluation (HellaSwag, ARC, MMLU, TruthfulQA, GSM8K)</li> <li>GuideLLM: Performance benchmarking (TTFT, ITL, throughput, latency)</li> </ul> <p>Container images:</p> <ul> <li><code>quay.io/eval-hub/community-lighteval:latest</code></li> <li><code>quay.io/eval-hub/community-guidellm:latest</code></li> </ul>"},{"location":"getting-started/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph users[\"Users\"]\n        DEV[Developer&lt;br/&gt;Python SDK]\n        OPS[Operator&lt;br/&gt;REST API]\n    end\n\n    subgraph server[\"EvalHub Server\"]\n        API[REST API&lt;br/&gt;Port 8080]\n        ORCH[Job Orchestrator]\n        DB[(PostgreSQL)]\n    end\n\n    subgraph cluster[\"Kubernetes/OpenShift\"]\n        subgraph job[\"Evaluation Job Pod\"]\n            ADAPTER[Adapter Container&lt;br/&gt;Framework Executor]\n            SIDECAR[Sidecar Container&lt;br/&gt;Status Reporter]\n        end\n        CM[ConfigMap&lt;br/&gt;JobSpec]\n    end\n\n    subgraph external[\"External Services\"]\n        MODEL[Model Endpoint&lt;br/&gt;OpenAI, vLLM, etc.]\n        OCI[OCI Registry&lt;br/&gt;Artifacts]\n    end\n\n    DEV --&gt; API\n    OPS --&gt; API\n    API --&gt; ORCH\n    API --&gt; DB\n    ORCH --&gt; job\n    ORCH --&gt; CM\n    CM --&gt; ADAPTER\n    ADAPTER --&gt; MODEL\n    ADAPTER --&gt; SIDECAR\n    SIDECAR --&gt; API\n    SIDECAR --&gt; OCI\n\n    style server fill:#e3f2fd\n    style cluster fill:#fff3e0\n    style job fill:#f3e5f5</code></pre>"},{"location":"getting-started/overview/#data-flow","title":"Data Flow","text":"<ol> <li>Client submits evaluation via SDK or REST API</li> <li>Server creates Kubernetes Job with adapter container and sidecar</li> <li>ConfigMap mounted with JobSpec at <code>/meta/job.json</code></li> <li>Adapter loads JobSpec, runs evaluation, reports progress via callbacks</li> <li>Sidecar receives callbacks, forwards to server, persists artifacts to OCI registry</li> <li>Server stores results in PostgreSQL, returns status to client</li> </ol>"},{"location":"getting-started/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/overview/#providers","title":"Providers","text":"<p>Evaluation providers represent evaluation frameworks or services.</p> <p>Example providers:</p> <ul> <li><code>lm_evaluation_harness</code> - EleutherAI's evaluation framework</li> <li><code>ragas</code> - RAG evaluation framework</li> <li><code>garak</code> - LLM vulnerability scanner</li> <li><code>guidellm</code> - Performance benchmarking</li> </ul> <p>Each provider exposes a set of benchmarks.</p>"},{"location":"getting-started/overview/#benchmarks","title":"Benchmarks","text":"<p>Benchmarks are specific evaluation tasks within a provider.</p> <p>Example benchmarks:</p> <ul> <li><code>mmlu</code> (lm_evaluation_harness) - Multitask accuracy</li> <li><code>humaneval</code> (lm_evaluation_harness) - Code generation</li> <li><code>performance_test</code> (guidellm) - Throughput and latency</li> </ul>"},{"location":"getting-started/overview/#collections","title":"Collections","text":"<p>Collections are curated sets of benchmarks with weighted scoring.</p> <p>Example collection: </p><pre><code>collection_id: healthcare_safety_v1\nbenchmarks:\n  - benchmark_id: mmlu_medical\n    provider_id: lm_evaluation_harness\n    weight: 2.0\n  - benchmark_id: truthfulqa\n    provider_id: lm_evaluation_harness\n    weight: 1.5\n  - benchmark_id: toxicity\n    provider_id: garak\n    weight: 1.0\n</code></pre><p></p> <p>Collections enable domain-specific evaluation with a single API call.</p>"},{"location":"getting-started/overview/#jobs","title":"Jobs","text":"<p>Jobs are Kubernetes Job resources that execute evaluations in isolated environments. When an evaluation is submitted, the server creates a Job which progresses through a well-defined lifecycle: it begins in a Pending state while waiting for pod scheduling, transitions to Running when the adapter starts executing the evaluation, and completes with either a Completed status for successful evaluations or Failed when errors are encountered.</p>"},{"location":"getting-started/overview/#adapters","title":"Adapters","text":"<p>Adapters are containerised applications that bridge evaluation frameworks with EvalHub. Each adapter loads its JobSpec from a mounted ConfigMap, executes framework-specific evaluation logic, and reports progress through a standardised callback interface. Once evaluation completes, the adapter persists artifacts to an OCI registry and returns structured JobResults to the server, enabling consistent integration across diverse evaluation frameworks.</p>"},{"location":"getting-started/overview/#deployment-models","title":"Deployment Models","text":""},{"location":"getting-started/overview/#local-development","title":"Local Development","text":"<p>The local development model runs the server directly on your development machine at http://localhost:8080 without requiring Kubernetes infrastructure. Using SQLite for storage, this environment enables rapid iteration for server development, API testing, SDK development, and adapter prototyping.</p>"},{"location":"getting-started/overview/#openshift-production","title":"OpenShift Production","text":"<p>OpenShift deployments provide production-ready evaluation workflows through Kubernetes-native orchestration. The platform automatically manages job lifecycles, enforces resource isolation and limits, supports horizontal pod autoscaling, and integrates comprehensive monitoring and logging. This model is ideal for production model evaluation, CI/CD integration, multi-tenant evaluation scenarios, and large-scale benchmarking operations.</p>"},{"location":"getting-started/overview/#kubernetes-generic","title":"Kubernetes (Generic)","text":"<p>Generic Kubernetes deployments use standard manifests compatible with any conformant cluster, enabling self-managed infrastructure with flexible configuration. This deployment model suits on-premises installations, custom Kubernetes distributions, and air-gapped environments where organisations manage their own infrastructure.</p>"},{"location":"getting-started/overview/#why-evalhub","title":"Why EvalHub?","text":"<p>EvalHub provides a unified interface for evaluating models across heterogeneous frameworks using a consistent API, eliminating the need for framework-specific integration code. Its Kubernetes-native architecture leverages existing orchestration capabilities for job management, resource allocation, and scaling without requiring custom infrastructure. The adapter pattern enables extensibility, allowing new evaluation frameworks to integrate without modifying the core server. Production deployments benefit from structured logging, Prometheus metrics, and PostgreSQL persistence, providing the observability required for confident operation.</p>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Installation - Install server and SDK components</li> <li>Quick Start - Run your first evaluation end-to-end</li> <li>Server Documentation - Understand the Go server</li> <li>SDK Documentation - Use the Python SDK</li> <li>Contrib Documentation - Explore community adapters</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#quick-start","title":"Quick Start","text":"<p>Get up and running with EvalHub in minutes. This guide uses GuideLLM as an example, but the same workflow applies to any evaluation provider.</p>"},{"location":"getting-started/quickstart/#step-1-start-model-server","title":"Step 1: Start Model Server","text":"OpenShift (vLLM)Local (Ollama) <p>Deploy vLLM on OpenShift:</p> <pre><code># Example vLLM deployment\noc apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm\n  template:\n    metadata:\n      labels:\n        app: vllm\n    spec:\n      containers:\n      - name: vllm\n        image: vllm/vllm-openai:latest\n        args:\n        - --model\n        - meta-llama/Llama-3.2-1B-Instruct\n        - --port\n        - \"8000\"\n        ports:\n        - containerPort: 8000\nEOF\n</code></pre> <p>Start Ollama for local development:</p> <pre><code># Install Ollama (if not already installed)\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull and run a model\nollama run qwen2.5:1.5b\n</code></pre> <p>Ollama serves at <code>http://localhost:11434/v1</code> (OpenAI-compatible).</p>"},{"location":"getting-started/quickstart/#step-2-install-client-sdk","title":"Step 2: Install Client SDK","text":"<pre><code>pip install eval-hub-sdk[client]\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-submit-evaluation","title":"Step 3: Submit Evaluation","text":"<p>Use the Python client to submit an evaluation:</p> <pre><code>from evalhub.client import EvalHubClient\nfrom evalhub.models.api import ModelConfig, BenchmarkSpec\n\n# Connect to EvalHub server\nclient = EvalHubClient(base_url=\"http://localhost:8080\")\n\n# Submit evaluation\njob = client.submit_evaluation(\n    model=ModelConfig(\n        url=\"http://localhost:11434/v1\",  # or http://vllm-server:8000/v1 for k8s\n        name=\"qwen2.5:1.5b\"\n    ),\n    benchmarks=[\n        BenchmarkSpec(\n            benchmark_id=\"performance_test\",\n            provider_id=\"guidellm\",\n            config={\n                \"profile\": \"constant\",\n                \"rate\": 5,\n                \"max_seconds\": 10,\n                \"max_requests\": 20,\n                \"data\": \"prompt_tokens=50,output_tokens=20\",\n                \"warmup\": \"0\"\n            }\n        )\n    ]\n)\n\nprint(f\"Job submitted: {job.job_id}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-view-results","title":"Step 4: View Results","text":"<p>Check the job status and retrieve results:</p> <pre><code># Get job status\nstatus = client.get_job_status(job.job_id)\nprint(f\"Status: {status.status}\")\nprint(f\"Progress: {status.progress}\")\n\n# Wait for completion and get results\nif status.status == \"completed\":\n    results = client.get_job_results(job.job_id)\n    print(f\"Results: {results}\")\n</code></pre> <p>Example REST response:</p> <pre><code>{\n  \"job_id\": \"quickstart-001\",\n  \"status\": \"completed\",\n  \"progress\": 1.0,\n  \"results\": {\n    \"benchmark_id\": \"performance_test\",\n    \"provider_id\": \"guidellm\",\n    \"metrics\": {\n      \"requests_per_second\": 5.0,\n      \"input_tokens_per_second\": 263.2,\n      \"output_tokens_per_second\": 105.3,\n      \"total_requests\": 20,\n      \"mean_ttft_ms\": 45.3,\n      \"mean_itl_ms\": 12.1\n    },\n    \"overall_score\": 0.95\n  }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#explore-other-providers","title":"Explore Other Providers","text":"<p>EvalHub supports multiple evaluation frameworks:</p> <ul> <li>lm_evaluation_harness: Standard LLM benchmarks (MMLU, HellaSwag, ARC, etc.)</li> <li>guidellm: Performance benchmarking</li> <li>ragas: RAG evaluation</li> <li>garak: LLM vulnerability scanning</li> </ul> <pre><code># List all available providers\nproviders = client.list_providers()\nfor provider in providers:\n    print(f\"{provider.provider_id}: {provider.description}\")\n</code></pre>"},{"location":"getting-started/quickstart/#try-different-benchmarks","title":"Try Different Benchmarks","text":"<p>Each provider offers multiple benchmarks:</p> <pre><code># List benchmarks for a provider\nbenchmarks = client.list_benchmarks(provider_id=\"lm_evaluation_harness\")\nfor benchmark in benchmarks:\n    print(f\"{benchmark.benchmark_id}: {benchmark.name}\")\n</code></pre>"},{"location":"getting-started/quickstart/#use-collections","title":"Use Collections","text":"<p>Run curated benchmark collections:</p> <pre><code># Submit evaluation using a collection\njob = client.submit_evaluation(\n    model=ModelConfig(url=\"...\", name=\"...\"),\n    collection_id=\"healthcare_safety_v1\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#model-server-not-responding","title":"Model Server Not Responding","text":"vLLMOllama <p>Check vLLM pod status:</p> <pre><code># Check pod status\noc get pods -l app=vllm\n\n# Check logs\noc logs -l app=vllm --tail=50\n\n# Check service\noc get svc vllm-server\n</code></pre> <p>Check if Ollama is running:</p> <pre><code># Check if Ollama is running\ncurl http://localhost:11434/v1/models\n\n# Check Ollama service\nollama list\n\n# Restart Ollama\nollama serve\n</code></pre>"},{"location":"getting-started/quickstart/#job-stuck-in-pending","title":"Job Stuck in Pending","text":"<p>Check server logs:</p> <pre><code># Local\neval-hub-server --log-level=debug\n\n# Kubernetes\nkubectl logs -n evalhub deployment/evalhub-server\n</code></pre>"},{"location":"getting-started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Installation Guide - Complete installation instructions</li> <li>Server Configuration - Configure the EvalHub server</li> <li>API Reference - REST API documentation</li> <li>Provider Configuration - Add custom providers</li> </ul>"},{"location":"reference/api/","title":"REST API","text":""},{"location":"reference/api/#api-reference","title":"API Reference","text":"<p>API reference for eval-hub adapters.</p>"},{"location":"reference/api/#coming-soon","title":"Coming Soon","text":"<p>Detailed API documentation is in progress.</p>"},{"location":"reference/api/#core-classes","title":"Core Classes","text":""},{"location":"reference/api/#frameworkadapter","title":"FrameworkAdapter","text":"<p>Base class for all adapters.</p> <pre><code>class FrameworkAdapter:\n    def run_benchmark_job(\n        self,\n        job_spec: JobSpec,\n        callbacks: JobCallbacks\n    ) -&gt; JobResults:\n        \"\"\"Run a benchmark job.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/#jobspec","title":"JobSpec","text":"<p>Job configuration.</p> <pre><code>@dataclass\nclass JobSpec:\n    job_id: str\n    benchmark_id: str\n    model: ModelConfig\n    benchmark_config: Dict[str, Any]\n</code></pre>"},{"location":"reference/api/#jobresults","title":"JobResults","text":"<p>Evaluation results.</p> <pre><code>@dataclass\nclass JobResults:\n    job_id: str\n    benchmark_id: str\n    metrics: Dict[str, Any]\n    overall_score: Optional[float]\n</code></pre> <p>For complete API documentation, see the evalhub-sdk source.</p>"},{"location":"reference/sdk-client/","title":"Python SDK","text":""},{"location":"reference/sdk-client/#python-client-sdk","title":"Python Client SDK","text":"<p>The EvalHub Python SDK provides a high-level interface for interacting with the EvalHub REST API. The SDK supports both synchronous and asynchronous operations through separate client classes.</p>"},{"location":"reference/sdk-client/#installation","title":"Installation","text":"<p>Install the SDK from PyPI:</p> <pre><code>pip install eval-hub-sdk[client]\n</code></pre> <p>For development installations:</p> <pre><code>git clone https://github.com/eval-hub/eval-hub-sdk.git\ncd eval-hub-sdk\npip install -e .[client]\n</code></pre>"},{"location":"reference/sdk-client/#quick-start","title":"Quick Start","text":"SynchronousAsynchronous <pre><code>from evalhub.client import SyncEvalHubClient\nfrom evalhub.models.api import ModelConfig, EvaluationRequest\n\n# Create client\nwith SyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    # List available benchmarks\n    benchmarks = client.benchmarks.list()\n    print(f\"Found {len(benchmarks)} benchmarks\")\n\n    # Submit evaluation job\n    job = client.jobs.submit(\n        EvaluationRequest(\n            benchmark_id=\"mmlu\",\n            model=ModelConfig(\n                url=\"https://api.openai.com/v1\",\n                name=\"gpt-4\"\n            )\n        )\n    )\n\n    # Monitor job status\n    status = client.jobs.get(job.job_id)\n    print(f\"Job {job.job_id}: {status.status}\")\n</code></pre> <pre><code>from evalhub.client import AsyncEvalHubClient\nfrom evalhub.models.api import ModelConfig, EvaluationRequest\n\n# Create async client\nasync with AsyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    # List available benchmarks\n    benchmarks = await client.benchmarks.list()\n    print(f\"Found {len(benchmarks)} benchmarks\")\n\n    # Submit evaluation job\n    job = await client.jobs.submit(\n        EvaluationRequest(\n            benchmark_id=\"mmlu\",\n            model=ModelConfig(\n                url=\"https://api.openai.com/v1\",\n                name=\"gpt-4\"\n            )\n        )\n    )\n\n    # Monitor job status\n    status = await client.jobs.get(job.job_id)\n    print(f\"Job {job.job_id}: {status.status}\")\n</code></pre>"},{"location":"reference/sdk-client/#client-configuration","title":"Client Configuration","text":"<p>Both <code>SyncEvalHubClient</code> and <code>AsyncEvalHubClient</code> support the following configuration options:</p> <pre><code>client = SyncEvalHubClient(\n    base_url=\"http://localhost:8080\",      # EvalHub service URL\n    auth_token=None,                        # Optional authentication token\n    timeout=30.0,                           # Request timeout in seconds\n    max_retries=3,                          # Maximum retry attempts\n    verify_ssl=True,                        # SSL certificate verification\n    retry_initial_delay=1.0,                # Initial retry delay\n    retry_max_delay=60.0,                   # Maximum retry delay\n    retry_backoff_factor=2.0,               # Exponential backoff multiplier\n    retry_randomization=True                # Add jitter to retries\n)\n</code></pre>"},{"location":"reference/sdk-client/#resource-operations","title":"Resource Operations","text":"<p>The SDK provides a resource-based API for interacting with different EvalHub entities:</p>"},{"location":"reference/sdk-client/#providers","title":"Providers","text":"<p>List and retrieve evaluation providers:</p> <pre><code># List all providers\nproviders = client.providers.list()\n\n# Get specific provider\nprovider = client.providers.get(\"lm_evaluation_harness\")\n\n# Get provider with benchmarks\nprovider = client.providers.get(\"lm_evaluation_harness\", include_benchmarks=True)\n</code></pre>"},{"location":"reference/sdk-client/#benchmarks","title":"Benchmarks","text":"<p>Discover and filter available benchmarks:</p> <pre><code># List all benchmarks\nbenchmarks = client.benchmarks.list()\n\n# Filter by category\nmath_benchmarks = client.benchmarks.list(category=\"math\")\n\n# Filter by provider\nlmeval_benchmarks = client.benchmarks.list(provider_id=\"lm_evaluation_harness\")\n\n# Get specific benchmark\nbenchmark = client.benchmarks.get(\"mmlu\")\n</code></pre>"},{"location":"reference/sdk-client/#collections","title":"Collections","text":"<p>Work with benchmark collections:</p> <pre><code># List all collections\ncollections = client.collections.list()\n\n# Get specific collection\ncollection = client.collections.get(\"healthcare_safety_v1\")\n\n# Collections include benchmark lists\nfor benchmark_id in collection.benchmark_ids:\n    print(f\"  - {benchmark_id}\")\n</code></pre>"},{"location":"reference/sdk-client/#jobs","title":"Jobs","text":"<p>Submit and manage evaluation jobs:</p> <pre><code>from evalhub.models.api import EvaluationRequest, ModelConfig\n\n# Submit evaluation job\njob = client.jobs.submit(\n    EvaluationRequest(\n        benchmark_id=\"mmlu\",\n        model=ModelConfig(\n            url=\"https://api.openai.com/v1\",\n            name=\"gpt-4\"\n        ),\n        num_examples=100,                    # Optional: limit examples\n        benchmark_config={                   # Optional: custom config\n            \"num_few_shot\": 5,\n            \"random_seed\": 42\n        }\n    )\n)\n\n# Get job status\nstatus = client.jobs.get(job.job_id)\n\n# List all jobs\nall_jobs = client.jobs.list()\n\n# Filter jobs by status\nfrom evalhub.models.api import JobStatus\n\nrunning_jobs = client.jobs.list(status=JobStatus.RUNNING)\ncompleted_jobs = client.jobs.list(status=JobStatus.COMPLETED)\n\n# Wait for job completion (blocking)\nfinal_status = client.jobs.wait_for_completion(\n    job.job_id,\n    timeout=3600,        # Maximum wait time in seconds\n    poll_interval=5.0    # Check every 5 seconds\n)\n\n# Cancel a job\nclient.jobs.cancel(job.job_id)\n</code></pre>"},{"location":"reference/sdk-client/#complete-examples","title":"Complete Examples","text":""},{"location":"reference/sdk-client/#example-1-run-evaluation-and-get-results","title":"Example 1: Run Evaluation and Get Results","text":"<pre><code>from evalhub.client import SyncEvalHubClient\nfrom evalhub.models.api import ModelConfig, EvaluationRequest, JobStatus\n\nwith SyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    # Submit evaluation\n    job = client.jobs.submit(\n        EvaluationRequest(\n            benchmark_id=\"mmlu\",\n            model=ModelConfig(\n                url=\"https://api.openai.com/v1\",\n                name=\"gpt-4\"\n            ),\n            num_examples=100\n        )\n    )\n\n    print(f\"Job submitted: {job.job_id}\")\n\n    # Wait for completion\n    try:\n        result = client.jobs.wait_for_completion(\n            job.job_id,\n            timeout=3600,\n            poll_interval=10.0\n        )\n\n        if result.status == JobStatus.COMPLETED:\n            print(f\"\u2705 Evaluation completed!\")\n            print(f\"Results: {result.results}\")\n        elif result.status == JobStatus.FAILED:\n            print(f\"\u274c Evaluation failed: {result.error}\")\n\n    except TimeoutError:\n        print(f\"\u23f1\ufe0f Job did not complete within timeout\")\n        client.jobs.cancel(job.job_id)\n</code></pre>"},{"location":"reference/sdk-client/#example-2-list-available-benchmarks","title":"Example 2: List Available Benchmarks","text":"<pre><code>from evalhub.client import SyncEvalHubClient\n\nwith SyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    # Get all providers\n    providers = client.providers.list()\n\n    print(\"Available Evaluation Providers:\")\n    print(\"=\" * 50)\n\n    for provider in providers:\n        print(f\"\\n{provider.name}\")\n        print(f\"  ID: {provider.id}\")\n        print(f\"  Type: {provider.type}\")\n\n        # Get benchmarks for this provider\n        benchmarks = client.benchmarks.list(provider_id=provider.id)\n        print(f\"  Benchmarks ({len(benchmarks)}):\")\n\n        for benchmark in benchmarks[:5]:  # Show first 5\n            print(f\"    - {benchmark.id}: {benchmark.name}\")\n</code></pre>"},{"location":"reference/sdk-client/#example-3-monitor-multiple-jobs","title":"Example 3: Monitor Multiple Jobs","text":"<pre><code>from evalhub.client import SyncEvalHubClient\nfrom evalhub.models.api import ModelConfig, EvaluationRequest, JobStatus\nimport time\n\nwith SyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    # Submit multiple evaluations\n    benchmarks = [\"mmlu\", \"hellaswag\", \"truthfulqa\"]\n    jobs = []\n\n    for benchmark_id in benchmarks:\n        job = client.jobs.submit(\n            EvaluationRequest(\n                benchmark_id=benchmark_id,\n                model=ModelConfig(\n                    url=\"https://api.openai.com/v1\",\n                    name=\"gpt-4\"\n                )\n            )\n        )\n        jobs.append(job)\n        print(f\"Submitted {benchmark_id}: {job.job_id}\")\n\n    # Monitor all jobs\n    while jobs:\n        for job in jobs[:]:  # Copy list to allow removal\n            status = client.jobs.get(job.job_id)\n\n            if status.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]:\n                print(f\"\u2713 {status.benchmark_id}: {status.status}\")\n                jobs.remove(job)\n            else:\n                print(f\"\u22ef {status.benchmark_id}: {status.status} ({status.progress:.0%})\")\n\n        if jobs:\n            time.sleep(10)  # Wait before next check\n</code></pre>"},{"location":"reference/sdk-client/#example-4-async-client-with-multiple-concurrent-jobs","title":"Example 4: Async Client with Multiple Concurrent Jobs","text":"<pre><code>import asyncio\nfrom evalhub.client import AsyncEvalHubClient\nfrom evalhub.models.api import ModelConfig, EvaluationRequest\n\nasync def run_evaluation(client, benchmark_id):\n    \"\"\"Submit and wait for a single evaluation.\"\"\"\n    job = await client.jobs.submit(\n        EvaluationRequest(\n            benchmark_id=benchmark_id,\n            model=ModelConfig(\n                url=\"https://api.openai.com/v1\",\n                name=\"gpt-4\"\n            )\n        )\n    )\n\n    print(f\"Started {benchmark_id}: {job.job_id}\")\n\n    result = await client.jobs.wait_for_completion(\n        job.job_id,\n        timeout=3600,\n        poll_interval=5.0\n    )\n\n    print(f\"Completed {benchmark_id}: {result.status}\")\n    return result\n\nasync def main():\n    async with AsyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n        # Run multiple evaluations concurrently\n        benchmarks = [\"mmlu\", \"hellaswag\", \"truthfulqa\"]\n\n        tasks = [\n            run_evaluation(client, benchmark_id)\n            for benchmark_id in benchmarks\n        ]\n\n        # Wait for all to complete\n        results = await asyncio.gather(*tasks)\n\n        # Process results\n        for result in results:\n            print(f\"{result.benchmark_id}: {result.results}\")\n\n# Run async code\nasyncio.run(main())\n</code></pre>"},{"location":"reference/sdk-client/#error-handling","title":"Error Handling","text":"<p>The SDK raises standard HTTP exceptions for error cases:</p> <pre><code>import httpx\nfrom evalhub.client import SyncEvalHubClient, ClientError\n\nwith SyncEvalHubClient(base_url=\"http://localhost:8080\") as client:\n    try:\n        job = client.jobs.get(\"nonexistent-job-id\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            print(\"Job not found\")\n        elif e.response.status_code == 500:\n            print(\"Server error\")\n        else:\n            print(f\"HTTP error: {e.response.status_code}\")\n    except httpx.RequestError as e:\n        print(f\"Connection error: {e}\")\n    except ClientError as e:\n        print(f\"Client error: {e}\")\n</code></pre>"},{"location":"reference/sdk-client/#api-reference","title":"API Reference","text":""},{"location":"reference/sdk-client/#client-classes","title":"Client Classes","text":"<ul> <li><code>AsyncEvalHubClient</code> - Asynchronous client (requires <code>await</code>)</li> <li><code>SyncEvalHubClient</code> - Synchronous client (no <code>await</code> needed)</li> <li><code>EvalHubClient</code> - Alias for <code>AsyncEvalHubClient</code></li> </ul>"},{"location":"reference/sdk-client/#resource-classes","title":"Resource Classes","text":"<p>Each client provides access to these resources:</p> <ul> <li><code>client.providers</code> - Provider operations</li> <li><code>client.benchmarks</code> - Benchmark discovery</li> <li><code>client.collections</code> - Collection operations</li> <li><code>client.jobs</code> - Job submission and monitoring</li> </ul>"},{"location":"reference/sdk-client/#model-classes","title":"Model Classes","text":"<p>Key model classes from <code>evalhub.models.api</code>:</p> <ul> <li><code>ModelConfig</code> - Model server configuration</li> <li><code>EvaluationRequest</code> - Evaluation job request</li> <li><code>EvaluationJob</code> - Job status and results</li> <li><code>JobStatus</code> - Job status enum (PENDING, RUNNING, COMPLETED, FAILED, CANCELLED)</li> <li><code>Provider</code> - Provider information</li> <li><code>Benchmark</code> - Benchmark metadata</li> <li><code>Collection</code> - Benchmark collection</li> </ul>"},{"location":"reference/sdk-client/#best-practices","title":"Best Practices","text":""},{"location":"reference/sdk-client/#use-context-managers","title":"Use Context Managers","text":"<p>Always use context managers to ensure proper cleanup:</p> <pre><code># \u2705 Good - automatic cleanup\nwith SyncEvalHubClient() as client:\n    job = client.jobs.submit(request)\n\n# \u274c Bad - manual cleanup required\nclient = SyncEvalHubClient()\njob = client.jobs.submit(request)\nclient.close()  # Must remember to close\n</code></pre>"},{"location":"reference/sdk-client/#configure-retries","title":"Configure Retries","text":"<p>For production environments, configure retry behaviour:</p> <pre><code>client = SyncEvalHubClient(\n    base_url=\"https://evalhub.example.com\",\n    max_retries=5,\n    retry_initial_delay=2.0,\n    retry_max_delay=120.0,\n    retry_backoff_factor=2.0,\n    retry_randomization=True\n)\n</code></pre>"},{"location":"reference/sdk-client/#handle-timeouts","title":"Handle Timeouts","text":"<p>Set appropriate timeouts for long-running evaluations:</p> <pre><code># Long timeout for complex benchmarks\nresult = client.jobs.wait_for_completion(\n    job.job_id,\n    timeout=7200,  # 2 hours\n    poll_interval=30.0\n)\n</code></pre>"},{"location":"reference/sdk-client/#use-async-for-concurrency","title":"Use Async for Concurrency","text":"<p>For running multiple evaluations concurrently, use the async client:</p> <pre><code># Async allows concurrent operations\nasync with AsyncEvalHubClient() as client:\n    jobs = await asyncio.gather(\n        client.jobs.submit(request1),\n        client.jobs.submit(request2),\n        client.jobs.submit(request3)\n    )\n</code></pre>"},{"location":"reference/sdk-client/#see-also","title":"See Also","text":"<ul> <li>REST API Reference - Complete API documentation</li> <li>Creating Adapters - Build custom adapters</li> <li>Architecture - System architecture overview</li> </ul>"},{"location":"server/","title":"Overview","text":""},{"location":"server/#evalhub-server","title":"EvalHub Server","text":"<p>REST API orchestration service for managing LLM evaluation workflows.</p>"},{"location":"server/#what-is-the-evalhub-server","title":"What is the EvalHub Server?","text":"<p>The EvalHub Server is an open source Go application that provides a versioned REST API for evaluation job management, orchestrates Kubernetes Job creation and lifecycle management, and maintains a registry for discovering evaluation providers and benchmarks. It supports curated benchmark collections with weighted scoring, uses SQLite for local development and PostgreSQL for production, and includes structured logging, Prometheus metrics, and health checks for observability.</p>"},{"location":"server/#core-capabilities","title":"Core Capabilities","text":""},{"location":"server/#evaluation-job-management","title":"Evaluation Job Management","text":"<p>The server manages evaluation jobs through REST API endpoints, supporting job creation from benchmark specifications or curated collections, status monitoring, result retrieval, and job cancellation.</p>"},{"location":"server/#provider-and-benchmark-discovery","title":"Provider and Benchmark Discovery","text":"<p>The provider registry enables discovery of evaluation providers and their benchmarks, allowing clients to list registered providers, retrieve provider metadata, enumerate benchmarks, and access benchmark configuration schemas.</p>"},{"location":"server/#collection-management","title":"Collection Management","text":"<p>Collections provide curated benchmark sets with weighted scoring, supporting provider-based grouping and domain-specific configurations such as healthcare or finance compliance, all executable through a single API call.</p>"},{"location":"server/#rest-api","title":"REST API","text":"<p>The server exposes a versioned REST API at <code>/api/v1/</code> following RESTful resource-oriented design with JSON request and response bodies, standard HTTP status codes, and an OpenAPI 3.1.0 specification.</p>"},{"location":"server/#core-endpoints","title":"Core Endpoints","text":"<p>Evaluation Jobs: </p><pre><code>POST   /api/v1/evaluations/jobs        # Create evaluation job\nGET    /api/v1/evaluations/jobs        # List jobs\nGET    /api/v1/evaluations/jobs/{id}   # Get job details\nDELETE /api/v1/evaluations/jobs/{id}   # Cancel job\n</code></pre><p></p> <p>Providers &amp; Benchmarks: </p><pre><code>GET /api/v1/providers               # List providers\nGET /api/v1/providers/{id}          # Get provider details\nGET /api/v1/benchmarks              # List all benchmarks\nGET /api/v1/benchmarks/{id}         # Get benchmark details\n</code></pre><p></p> <p>Collections: </p><pre><code>GET /api/v1/collections             # List collections\nGET /api/v1/collections/{id}        # Get collection details\n</code></pre><p></p> <p>Health &amp; Metrics: </p><pre><code>GET /api/v1/health    # Health check\nGET /metrics          # Prometheus metrics\n</code></pre><p></p> <p>See API Reference for complete endpoint documentation.</p>"},{"location":"server/#configuration","title":"Configuration","text":"<p>Configuration loads from multiple sources with precedence: base configuration from <code>config/config.yaml</code>, environment variable overrides, and secrets from files for sensitive data.</p>"},{"location":"server/#example-configuration","title":"Example Configuration","text":"<p>config/config.yaml: </p><pre><code>service:\n  port: 8080\n\ndatabase:\n  host: localhost\n  port: 5432\n  name: eval_hub\n  user: eval_hub\n\nenv:\n  mappings:\n    service.port: PORT\n    database.host: DB_HOST\n\nsecrets:\n  dir: /var/secrets\n  mappings:\n    database.password: db_password\n</code></pre><p></p> <p>See Configuration for comprehensive reference.</p>"},{"location":"server/#observability","title":"Observability","text":""},{"location":"server/#logging","title":"Logging","text":"<p>The server uses structured JSON logging with automatic request enrichment, including timestamp, log level, message, request correlation ID, HTTP method and path, and client details. Each log entry captures the full request context for debugging and tracing.</p>"},{"location":"server/#health-checks","title":"Health Checks","text":"<p>The server provides health check endpoints at <code>/api/v1/health</code> for Kubernetes liveness and readiness probes, verifying server responsiveness and database connectivity to ensure the pod is ready to receive traffic.</p>"},{"location":"server/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> </ul>"}]}